# Машинное обучение: экзамен

Список теоретических вопросов для подготовки к экзамену:

- 1. Понятие машинного обучения. Отличие машинного обучения от других областей программирования.
    
    Машинное обучение (Machine Learning) - это направление искусственного интеллекта, которое занимается разработкой алгоритмов и моделей, способных автоматически учиться на данных и принимать решения на их основе. Машинное обучение используется в различных областях, таких как обработка естественного языка, компьютерное зрение, робототехника, биоинформатика и многих других. 
    
    Машинное обучение - это численная оптимизация параметрических моделей для описания определенного набора данных.
    
    В отличие от других областей программирования в машинном обучение нет явного описания алгоритма решения задачи. Используемая для решения численная модель находит наилучшие параметры для исходных данных, выявляя определенные закономерности.
    
    Дальше для воды:
    
    Машинное обучение можно разделить на три основные категории:
    
    1. Обучение с учителем (Supervised Learning):
    
    Обучение с учителем - это тип машинного обучения, в котором алгоритм обучается на помеченных данных, то есть данных, для которых известны правильные ответы. Задача алгоритма - научиться предсказывать правильные ответы для новых, непомеченных данных. Примерами задач обучения с учителем являются классификация и регрессия.
    
    1. Обучение без учителя (Unsupervised Learning):
    
    Обучение без учителя - это тип машинного обучения, в котором алгоритм обучается на непомеченных данных, то есть данных, для которых не известны правильные ответы. Задача алгоритма - найти шаблоны и структуру в данных, например, кластеры или ассоциации. Примерами задач обучения без учителя являются кластеризация и выделение признаков.
    
    1. Обучение с подкреплением (Reinforcement Learning):
    
    Обучение с подкреплением - это тип машинного обучения, в котором алгоритм обучается путем взаимодействия с окружающей средой. Алгоритм получает вознаграждение или наказание за свои действия и стремится максимизировать суммарное вознаграждение за все действия. Примерами задач обучения с подкреплением являются игры, управление роботами и оптимизация процессов.
    
    Примеры применения машинного обучения:
    
    - Обработка естественного языка: машинное обучение используется для разработки систем автоматического перевода, распознавания речи и анализа текстов.
    - Компьютерное зрение: машинное обучение используется для разработки систем распознавания изображений, видеоанализа и автономных транспортных средств.
    - Робототехника: машинное обучение используется для разработки роботов, способных выполнять сложные задачи, такие как сборка или манипуляции с предметами.
    - Биоинформатика: машинное обучение используется для анализа генетических данных, предсказания структуры белков и разработки лекарств.
- 2. Классификация задач машинного обучения. Примеры задач из различных классов.
    
    Классификация задач машинного обучения является важным аспектом в процессе разработки и применения алгоритмов машинного обучения. Существует несколько способов классификации задач машинного обучения, но один из наиболее распространенных способов - это классификация по типу выходных данных. 
    
    ### Обучение с учителем
    
    1. Первый самый распространенный тип - классификация
    
    **Классификация –** это процесс распознавания, понимания и группировки объектов по соответствующим категориям. В задаче классификации модель или программа использует предоставленный набор данных, чтобы научиться классифицировать новые наблюдения по различным классам или группам. В **бинарной классификации** цель состоит в том, чтобы классифицировать входные данные по двум взаимоисключающим категориям. Примеры алгоритмов, которые используются в бинарной классификации, — это логистическая регрессия, деревья решений, простой байесовский алгоритм и метод опорных векторов. **Логистическая регрессия** - это алгоритм бинарной классификации, основанный на применении линейной регрессии.  Результат работы логистической функции часто интерпретируется как вероятность отнесения объекта к положительному классу. Для четкой классификации обычно выбирают некоторое пороговое значение, обычно  0,5.
    
    **Многоклассовая классификация** направлена на то, чтобы предсказать, к какому классу принадлежит данный входной пример**.** В мультиклассовой классификации существует как минимум 2 взаимоисключающих метки класса, а экземпляры группируются в один из нескольких классов, вот почему будет более 2 исходов, а также потому, что нет понятия нормального или ненормального состояния, как в бинарной классификации. Примеры алгоритмов, которые используются для этой классификации, — это случайный лес, ступичный байес, k-ближайших соседей и повышение градиента. 
    
    Примерами задач классификации являются:
    
    - Распознавание рукописного текста: задача состоит в том, чтобы классифицировать изображения рукописных букв или цифр в соответствующие классы.
    - Классификация изображений: задача состоит в том, чтобы классифицировать изображения в заранее определенные классы, такие как "кот", "собака", "автомобиль" и т.д.
    - Классификация текстов: задача состоит в том, чтобы классифицировать тексты в заранее определенные классы, такие как "положительный отзыв", "отрицательный отзыв", "новость" и т.д.
    1. **Регрессия —** это процесс определения статистической взаимосвязи между зависимой переменной и одной или несколькими независимыми переменными. **Линейная регрессия** используется для нахождения линейной зависимости между зависимой переменной и одной или несколькими независимыми переменными с использованием наиболее подходящей прямой линии, называемой линией регрессии. Она делает прогнозы, вычисляя взвешенную сумму входных признаков. плюс константа, называемая смещением. В линейной регрессии зависимая переменная непрерывна, тогда как независимые переменные могут быть или не быть непрерывными. 
    
    Примерами задач регрессии являются:
    
    - Предсказание цены на недвижимость: задача состоит в том, чтобы предсказать цену на недвижимость на основе набора входных данных, таких как площадь, количество комнат, расположение и т.д.
    - Предсказание температуры: задача состоит в том, чтобы предсказать температуру на основе набора входных данных, таких как давление, влажность, время суток и т.д.
    - Предсказание продаж: задача состоит в том, чтобы предсказать продажи товара или услуги на основе набора входных данных, таких как цена, рекламные расходы, сезонность и т.д.
    
    ### Обучение без учителя
    
    1. **Кластеризация -** процесс организации группы объектов таким образом, что объекты в одной группе больше похожи друг на друга, чем на объекты в другой группе. В то время как классификация — это модель обучения с учителем, кластеризация — это метод машинного обучения без учителя, который не требует маркированных данных для алгоритмов кластеризации.
    - **Кластеризация на основе центроида**
    
    **Тип кластеризации на основе центроида организует данные в неиерархические кластеры**; этот тип кластеризации эффективен, но чувствителен к начальным условиям и выбросам.
    
    - **Кластеризация на основе плотности**
    
    Этот тип кластеризации **соединяет области с высокой плотностью в кластеры, что позволяет создавать распределения произвольной формы, если плотные области могут быть соединены.** Кластеризация на основе плотности затрудняет работу с данными различной плотности и больших размеров.
    
    - **Кластеризация на основе распределения**
    
    При **кластеризации на основе распределения предполагается, что данные состоят из распределений.** По мере увеличения расстояния от центра распределения вероятность того, что точка или часть данных принадлежит распределению, уменьшается.
    
    Примерами задач кластеризации являются:
    
    - Сегментация клиентов: задача состоит в том, чтобы объединить клиентов в группы или сегменты на основе их сходства или близости друг к другу по набору входных данных, таких как возраст, пол, доход, образование и т.д.
    - Анализ социальных сетей: задача состоит в том, чтобы объединить пользователей социальных сетей в группы или сообщества на основе их сходства или близости друг к другу по набору входных данных, таких как интересы, географическое расположение, возраст и т.д.
    - Анализ изображений: задача состоит в том, чтобы объединить изображения в группы или кластеры на основе их сходства или близости друг к другу по набору входных данных, таких как цвет, текстура, форма и т.д.
    
    ### Обучение с подкреплением
    
    Это более сложный вид обучения, где ИИ нужно не просто анализировать данные, а действовать самостоятельно в реальной среде — будь то улица, дом или видеоигра. Задача робота — свести ошибки к минимуму, за что он получает возможность продолжать работу без препятствий и сбоев.
    
    Обучение с подкреплением инженеры используют для беспилотников, роботов-пылесосов, торговли на фондовом рынке, управления ресурсами компании. Именно так алгоритму AlphaGo удалось обыграть чемпиона по игре Го: просчитать все возможные комбинации, как в шахматах, здесь было невозможно.
    
- 3. Основные понятия машинного обучения: набора данных, объекты, признаки, атрибуты, модели, параметры.
    
    **Обучающий набор данных** — это набор данных экземпляров, используемых во время обучающего процесса и используемых для настройки параметров
    
    **Объекты**:
    
    - Индивидуальные образцы данных в наборе данных.
    - Представляют собой сущности, которые мы хотим изучить или предсказать.
    
    Совокупность объектов, которые используются для того, чтобы построить модель, называется **обучающей выборкой**
    
    Объекты в машинном обучении описываются набором признаков.
    
    **Признаки** иначе называются независимыми переменными. Важно заметить, что признаки должны быть представлены в численном виде, потому что только так ими можно оперировать математически и обрабатывать компьютерно.
    
    **Атрибуты** (Attributes): Свойства признаков, которые могут влиять на результат моделирования.
    
    **Модель** машинного обучения — это приложение искусственного интеллекта (ИИ), которое дает возможность автоматически учиться и совершенствоваться на основе собственного опыта без явного участия человека.
    
    **Модель - это некоторая функция (в широком смысле, не обязательно математическая формула), которая имеет численные параметры. (лекция)**
    
    **Параметры** модели — параметры, которые изменяются и оптимизируются в процессе обучения модели и итоговые значения этих параметров являются результатом обучения модели.
    
    **Гиперпараметры модели** — параметры, значения которых задается до начала обучения модели и не изменяется в процессе обучения. У модели может не быть гиперпараметров.
    
    Примерами **гиперпараметров** могут служить **количество слоев нейронной сети**, а также **количество нейронов на каждом слое**. Примерами **параметров** могут служить **веса ребер нейронной сети**.
    
- 4. Структура и представление данных для машинного обучения.
    
    Для успешного применения алгоритмов машинного обучения необходимо правильно структурировать и представить данные. Обычно данные представляются в виде таблицы, где каждая строка соответствует отдельному объекту, а каждый столбец — признаку объекта. Важно учитывать тип данных (категориальные, числовые), обрабатывать пропущенные значения, масштабировать признаки и проводить другие подготовительные шаги.
    
    Кроме того, при обучении модели стоит разбить выборку на обучающие, тестовые и проверочные наборы.
    
    Если мы работаем с реальными датасетами, то стоит перед работой их подготовить:
    
    Модель, построенная на некачественных данных, будет неточной, а результаты её работы не получится использовать. Поэтому до обучения модели нужно провести подготовку исходных данных: очистить и привести к нужному формату.
    
    **1. Очистить данные.**
    
    При очистке данных удаляют устаревшие данные, дубликаты, аномалии, пропуски и ошибки. Но не всегда можно просто убрать все некачественные данные. Иногда их так много, что удаление повлияет на результаты машинного обучения, поэтому данные придётся редактировать.
    
    **2. Редактировать данные.**
    
    Данные могут быть записаны с ошибками или в разных форматах, поэтому их нужно корректировать. Например, в таблице может быть по-разному указано название одного и того же населённого пункта: «посёлок Заозёрный», «п. Заозёрный», «пос. Заозёрный», «поселок Заозерный». Модель будет воспринимать эти названия как разные значения, поэтому придётся привести записи к одному формату. Что касается числовых данных, то, чтобы привести их к единому формату, например, можно преобразовать значения в диапазон от 0 до 1.
    
    **3. Заполнить пропуски.**
    
    В работе с пропусками есть разные подходы. Их выбор зависит от видов и источников данных. Но сначала специалисту нужно разобраться, почему появились пропуски.
    
    Пропуски можно заполнить наиболее вероятным значением. Например, в форме для объявления о продаже автомобиля есть графа с информацией об участии в авариях. В графе есть два варианта ответа — «да» и «нет». Специалист, который проводит подготовку данных для машинного обучения, знает, что на основе статистики эту графу чаще пропускают в случае отсутствия аварий. Поэтому он может заполнить пропуски наиболее вероятным значением — «нет».
    
    Числовые показатели можно заменить, например, на усреднённые значения или построить алгоритм на основе взаимосвязей между показателями. С помощью такого алгоритма для каждого пропуска рассчитывается собственное значение.
    
    **4. Форматировать данные.**
    
    Инструменты машинного обучения, как правило, работают с данными в табличном формате. Поэтому набор чисел и текстовые записи преобразовывают в форматы .csv, .xls, .xlsx.
    
    Исходные данные в виде изображений тоже преобразовывают. Их можно конвертировать в один формат или сжать до определённого размера. К изображениям могут применять чёрно-белый или другой единый цветовой фильтр и обрезать. Например, если нужна информация из конкретного поля на скане документа, которое заполняется от руки, то все сканы можно автоматически обрезать на определённое количество пикселей со всех сторон.
    
    **5. Отобрать признаки данных.**
    
    Некоторые признаки могут быть сильно связаны между собой, поэтому приводят к утечке данных. Допустим, есть два признака — год рождения и возраст. Если выгрузка данных происходит в один день, то один из этих признаков стоит удалить.
    
    Отбор признаков также делают, чтобы снизить эффект шума. В этом случае удаляют те признаки, которые в наименьшей степени влияют на целевой показатель. Предположим, нужно сделать прогноз размера заработной платы. На этот показатель в большей степени повлияет сфера занятости и стаж работы, а вот день и месяц рождения сотрудника нет, значит, эти значения можно убрать.
    
- 5. Инструментальные средства машинного обучения.
    
    **Ответ:**
    
    Разделение процесса машинного обучения на три основных этапа:
    
    1. Сбор и обработка данных.
    2. Обучение и оценка модели.
    3. Использование обученной модели.
    
    Существует множество инструментальных средств машинного обучения. Вот некоторые из них:
    
    1. **Инструменты для сбора, обработки и визуализации данных**: pandas, Tableau, Power BI, Google Data Studio, Matplotlib.
    2. **Интерактивные среды разработки**: Jupyter Notebook, Kaggle, GoogleColaboratory
    3. **Фреймворки и библиотеки для общего машинного обучения**: NLTK, scikit-learn.
    4. **Фреймворки глубокого обучения и моделирования нейросетей**: PyTorch, TensorFlow.
    
    Выбор инструментальных средств зависит от задачи, сложности модели, размера и типа данных.
    
- 6. Задача регрессии: постановка, математическая формализация.
    
     **Ответ:**
    
    Регрессия - это тип задачи машинного обучения, при котором **модель обучается на основе обучающих данных, которые содержат информацию о числовых значениях**. Задача модели состоит в том, чтобы предсказать новые числовые значения на основе этой информации.
    
    **Постановка задачи регрессии**
    
    1. Регрессия - это задача машинного обучения с учителем, которая заключается в предсказании некоторой непрерывной величины.
    2. Для использования регрессионных моделей нужно, чтобы в датасете были характеристики объектов и “правильные” значения целевой переменной.
    3. Примеры регрессионных задач - предсказание цены акции, оценка объекта недвижимости.
    4. Задача регрессии основывается на предположении, что значение целевой переменной зависит от значения признаков.
    5. Регрессионная модель принимает набор значений и выдает предсказание значения целевой переменной.
    6. В качестве регрессионных моделей часто берут аналитические функции, например, линейную.
    
    математическая формализация:
    
    **Парная линейная регрессия** - используется когда хотим предсказать одно выходное значение (y), зависящее от одного входного значения (x).
    
    Самая ф-ия называется функцией гипотезы: $y_i = h_b(x) = b_0+b_1x_i$ - ур-е прямой
    
    **Функция гипотезы**
    
    1. Модель машинного обучения - это параметрическая функция.
    2. Задача обучения состоит в том, чтобы подобрать параметры модели таким образом, чтобы она лучше всего описывала обучающие данные.
    3. Парная линейная регрессия работает, если есть всего одна входящая переменная.
    4. Парная линейная регрессия - одна из самых простых моделей машинного обучения.
    5. Парная линейная регрессия соответствует множеству всех прямых на плоскости. Из них мы выбираем одну, наиболее подходящую.
    
    **Функция ошибки(потерь)**  - измеряет отклонение теор. значений (те, которые предсказывает модель) от эмпирических значений (те, которые есть в данных). Чем выше значение функции ошибки, тем хуже модель соответ. имеющимся данным. Если модель полностью соответ данным, то значение функции ошибки = 0. 
    
    ![Untitled](%D0%9C%D0%B0%D1%88%D0%B8%D0%BD%D0%BD%D0%BE%D0%B5%20%D0%BE%D0%B1%D1%83%D1%87%D0%B5%D0%BD%D0%B8%D0%B5%20%D1%8D%D0%BA%D0%B7%D0%B0%D0%BC%D0%B5%D0%BD%2004f25e7078ad4106b73d3ba8d4abfdb3/309803e1-6f03-4bd8-b9ff-8fbcae8d7cc5.png)
    
    Модели, обученные на маленьких объемах данных, будут иметь преимущество, поэтому берем не сумму, а среднее из отклонений (делим на кол-во точек данных).
    
    Данную функцию называют среднеквадратичной ошибкой (MSE). Среднее значение уменьшено вдвое для удобства вычисления градиентного спуска.
    
    ![Untitled](%D0%9C%D0%B0%D1%88%D0%B8%D0%BD%D0%BD%D0%BE%D0%B5%20%D0%BE%D0%B1%D1%83%D1%87%D0%B5%D0%BD%D0%B8%D0%B5%20%D1%8D%D0%BA%D0%B7%D0%B0%D0%BC%D0%B5%D0%BD%2004f25e7078ad4106b73d3ba8d4abfdb3/a1d53921-383c-4fda-9d03-8eb743855a58.png)
    
- 7. Метод градиентного спуска для парной линейной регрессии.
    
    **Метод градиентного спуска -** это итеративный оптимизационный алгоритм, используемый для нахождения минимума функции. В контексте линейной регрессии, он применяется для минимизации функции потерь, обычно MSE, и нахождения оптимальных значений параметров модели.
    
    **Парная линейная регрессия -** это метод, который моделирует зависимость между одной зависимой переменной (целевой переменной) ( y ) и одной независимой переменной (признаком) ( x ). Модель линейной регрессии имеет вид: y = $\beta0 + \beta_1 x + \epsilon$ 
    
    **Функция потерь**
    
    Целью линейной регрессии является минимизация функции потерь — среднеквадратичной ошибки:  $J(\beta_0, \beta_1) = \frac{1}{2m} \sum_{i=1}^m (h_{\beta}(x^{(i)}) - y^{(i)})^2$ 
    
    **Градиентный спуск**
    
    Метод градиентного спуска обновляет параметры модели в направлении, противоположном градиенту функции потерь. Обновление параметров \( \beta_0 \) и \( \beta_1 \) выполняется следующим образом:
    
    *1. Инициализация:* Задайте начальные значения $\beta_0$  и $\beta_1$ .
    
    *2. Обновление параметров:*
    
    $\beta_0 := \beta_0 - \alpha \frac{\partial J(\beta_0, \beta_1)}{\partial \beta_0}$
    
    $\beta_1 := \beta_1 - \alpha \frac{\partial J(\beta_0, \beta_1)}{\partial \beta_1}$
    
    где $\alpha$— скорость обучения.
    
    *3. Градиенты функции потерь:*
    
    $\frac{\partial J(\beta_0, \beta_1)}{\partial \beta_0} = \frac{1}{m} \sum_{i=1}^m (h_{\beta}(x^{(i)}) - y^{(i)})$ 
    
    $\frac{\partial J(\beta_0, \beta_1)}{\partial \beta_1} = \frac{1}{m} \sum_{i=1}^m (h_{\beta}(x^{(i)}) - y^{(i)}) x^{(i)}$
    
    **Итеративный процесс**
    
    Процесс продолжается до тех пор, пока значения параметров не сойдутся к оптимальным значениям, или пока не будет достигнуто максимальное количество итераций.
    
- 8. Понятие функции ошибки: требования, использование, примеры.
    
    **Функция ошибки** - это математический инструмент, используемый для измерения разницы между предсказанными значениями модели и фактическими (истинными) значениями. Она служит основой для обучения моделей машинного обучения и оценки их производительности.
    
    **Требования к функции ошибки**
    
    *1. Непрерывность и дифференцируемость:* Для большинства оптимизационных методов, таких как градиентный спуск, функция ошибки должна быть непрерывной и дифференцируемой, чтобы можно было вычислить градиенты и минимизировать функцию.
    
    *2. Невозрастающая по мере улучшения модели:* Функция ошибки должна уменьшаться по мере того, как модель становится лучше в своих предсказаниях.
    
    *3. Адекватное отражение цели модели:* Функция ошибки должна соответствовать целям задачи. Например, в задачах классификации часто используются другие функции потерь, чем в задачах регрессии.
    
    *4. Масштабируемость:* Функция ошибки должна быть способна масштабироваться на большие наборы данных и сложные модели.
    
    **Использование функции ошибки**
    
    *1. Оценка модели:* Функция ошибки используется для оценки качества модели. Меньшее значение функции ошибки указывает на лучшее соответствие предсказаний модели фактическим значениям.
    
    *2. Обучение модели:* Во время обучения модели функция ошибки используется для корректировки параметров модели с целью минимизации ошибки. Это достигается с помощью оптимизационных алгоритмов, таких как градиентный спуск.
    
    *3. Регуляризация:* В функцию ошибки часто включают дополнительные термины для регуляризации, чтобы предотвратить переобучение модели.
    
    **Примеры функций ошибки**
    
    *1. Среднеквадратичная ошибка (Mean Squared Error)* 
    
     $\text{MSE} = \frac{1}{m} \sum_{i=1}^m (y_i - \hat{y}_i)^2 \$  З: функция потер и mse отличаются
    
    где $y_i$ — истинное значение,
    
    $\hat{y}_i$— предсказанное значение,
    
     $m$ — количество наблюдений.
    
    MSE часто используется из-за своей математической простоты и дифференцируемости, что позволяет эффективно применять градиентный спуск.
    
    *2. Средняя абсолютная ошибка (Mean Absolute Error)*
    
    $\text{MAE} = \frac{1}{m} \sum_{i=1}^m |y_i - \hat{y}_i|$
    
    MAE измеряет среднюю абсолютную разницу между предсказанными и истинными значениями и менее чувствительна к выбросам, чем MSE.
    
- 9. Множественная и нелинейная регрессии.
    
    **Множественная регрессия**
    
    Множественная регрессия - это обобщение линейной регрессии на случай, когда у нас есть более одной независимой переменной. Модель множественной регрессии можно записать следующим образом:
    
    y=β0+β1x1+β2x2+…+βnxn
    
    где:
    
    - y - зависимая переменная,
    - x1,x2,…,xn - независимые переменные,
    - β0,β1,…,βn - параметры модели,
    
    **Нелинейная регрессия**
    
    Нелинейная регрессия используется, когда связь между зависимой переменной и одной или несколькими независимыми переменными не может быть адекватно описана линейной моделью. Нелинейные модели могут принимать различные формы, такие как полиномиальные, экспоненциальные, логарифмические и т.д. Общий вид нелинейной модели:
    
    y=f(x1,x2,…,xn;θ)
    
    где:
    
    - f - нелинейная функция,
    - θ - вектор параметров модели.
- 10. Нормализация признаков в задачах регрессии.
    
    **Причины нормализации признаков**
    
    1. Ускорение сходимости: Многие алгоритмы оптимизации сходятся быстрее при нормализованных данных.
    2. Улучшение численной стабильности: Ненормализованные данные могут привести к численным проблемам из-за больших разниц в масштабах признаков.
    3. Сравнимость коэффициентов: Нормализация позволяет сравнивать важность различных признаков в линейных моделях.
    
    **Основные методы нормализации**
    
    1. Стандартизация (Standardization):
    Приводит признаки к виду со средним 0 и стандартным отклонением 1.
        
        x′=(x−μ) / σ
        
        где μ — среднее значение признака, а σ — его стандартное отклонение.
        
    2. Масштабирование (Min-Max Scaling):
    Приводит признаки к заданному диапазону, обычно от 0 до 1.
        
        x′=(x−xmin) ⁡/ (xmax⁡−xmin⁡)
        
        где xmin и xmax — минимальное и максимальное значения признака соответственно.
        
    3. Масштабирование по максимальному абсолютному значению (MaxAbs Scaling):
    Приводит признаки к диапазону от -1 до 1.
        
        x′=x / ∣xmax⁡∣
        
        где ∣xmax⁡∣ — абсолютное максимальное значение признака.
        
- 11. Задача классификации: постановка, математическая формализация.
    
    Классификация в машинном обучении - задача отнесения объекта по совокупности его характеристик к одному из заранее известных классов.
    
    - Классификация - это задача машинного обучения, которая выражается в предсказании дискретного значения.
    - Классификация - это задача обучения с учителем, поэтому в датасете должны быть “правильные ответы” - значения целевой переменной.
    - Классификация - самая распространенная задача машинного обучения на практике.
    - Классификация бывает бинарной и множественной, одноклассовой и мультиклассовой.
    - Примеры задач классификации - распознавание объектов, генерация текстов, подбор тематики текстов, идентификация объектов на изображениях, распознавание речи, машинный перевод и так далее.
    - Почти любую практическую задачу машинного обучения можно сформулировать как задачу классификации.
    
    Математическая формализация задачи классификации может быть представлена следующим образом:
    
    Пусть дано множество объектов X = {x1, x2, ..., xn}, где каждый объект xi представлен набором признаков (атрибутов) xi = (xi1, xi2, ..., xim). Кроме того, каждый объект xi помечен меткой класса yi ∈ Y = {c1, c2, ..., ck}, где Y - множество всех возможных классов.
    
    Задача классификации состоит в том, чтобы построить модель (классификатор) f: X → Y, которая позволит предсказывать метку класса y для новых, не помеченных объектов x ∈ X на основе их признаков.
    
- 12. Метод градиентного спуска для задач классификации.
    
    **Градиентный спуск** — это алгоритм, который часто применяется в машинном обучении и искусственном интеллекте для минимизации функции потерь, измеряющей, насколько далеко предсказания модели от фактических результатов.
    Мы выбираем функцию гипотезы h(x) в качестве сигмоидной функции. Функция гипотезы аппроксимирует расчетную вероятность того, что фактический результат будет равен 1.
    
    ![Untitled](%D0%9C%D0%B0%D1%88%D0%B8%D0%BD%D0%BD%D0%BE%D0%B5%20%D0%BE%D0%B1%D1%83%D1%87%D0%B5%D0%BD%D0%B8%D0%B5%20%D1%8D%D0%BA%D0%B7%D0%B0%D0%BC%D0%B5%D0%BD%2004f25e7078ad4106b73d3ba8d4abfdb3/Untitled.png)
    
    Функция гипотезы рассчитывает вероятность принадлежности объекта к классу по входным параметрам.
    
    **Функция затрат(потерь)** в задачи классификации представлена логарифмической функцией и представлена для двух исходов (h_**θ -** функция гипотезы). Мы используем функцию затрат для измерения того, насколько прогнозы модели близки к фактическим результатам. 
    
     
    Для каждого класса мы можем рассчитать стоимость как:
    
    ![Untitled](%D0%9C%D0%B0%D1%88%D0%B8%D0%BD%D0%BD%D0%BE%D0%B5%20%D0%BE%D0%B1%D1%83%D1%87%D0%B5%D0%BD%D0%B8%D0%B5%20%D1%8D%D0%BA%D0%B7%D0%B0%D0%BC%D0%B5%D0%BD%2004f25e7078ad4106b73d3ba8d4abfdb3/Untitled%201.png)
    
    ![Untitled](%D0%9C%D0%B0%D1%88%D0%B8%D0%BD%D0%BD%D0%BE%D0%B5%20%D0%BE%D0%B1%D1%83%D1%87%D0%B5%D0%BD%D0%B8%D0%B5%20%D1%8D%D0%BA%D0%B7%D0%B0%D0%BC%D0%B5%D0%BD%2004f25e7078ad4106b73d3ba8d4abfdb3/Untitled%202.png)
    
    Общая **функция затрат** для одного наблюдения: y - значение целевой переменной, категория рассматриваемого объекта
    
    ![Untitled](%D0%9C%D0%B0%D1%88%D0%B8%D0%BD%D0%BD%D0%BE%D0%B5%20%D0%BE%D0%B1%D1%83%D1%87%D0%B5%D0%BD%D0%B8%D0%B5%20%D1%8D%D0%BA%D0%B7%D0%B0%D0%BC%D0%B5%D0%BD%2004f25e7078ad4106b73d3ba8d4abfdb3/Untitled%203.png)
    
    Для m наблюдений:
    
    ![Untitled](%D0%9C%D0%B0%D1%88%D0%B8%D0%BD%D0%BD%D0%BE%D0%B5%20%D0%BE%D0%B1%D1%83%D1%87%D0%B5%D0%BD%D0%B8%D0%B5%20%D1%8D%D0%BA%D0%B7%D0%B0%D0%BC%D0%B5%D0%BD%2004f25e7078ad4106b73d3ba8d4abfdb3/Untitled%204.png)
    
    Принцип работы градиентного спуска:
    
    1) По функции ошибки мы находим частные производные по каждому из параметров. Образуется вектор новых значений признаков
    
    ![Untitled](%D0%9C%D0%B0%D1%88%D0%B8%D0%BD%D0%BD%D0%BE%D0%B5%20%D0%BE%D0%B1%D1%83%D1%87%D0%B5%D0%BD%D0%B8%D0%B5%20%D1%8D%D0%BA%D0%B7%D0%B0%D0%BC%D0%B5%D0%BD%2004f25e7078ad4106b73d3ba8d4abfdb3/Untitled%205.png)
    
    2) Далее обновляем веса признаков (b), вычитая из них произведение градиента и скорости обучения.
    
    ![Untitled](%D0%9C%D0%B0%D1%88%D0%B8%D0%BD%D0%BD%D0%BE%D0%B5%20%D0%BE%D0%B1%D1%83%D1%87%D0%B5%D0%BD%D0%B8%D0%B5%20%D1%8D%D0%BA%D0%B7%D0%B0%D0%BC%D0%B5%D0%BD%2004f25e7078ad4106b73d3ba8d4abfdb3/Untitled%206.png)
    
    3) Итерации выполняются до тех пор, пока функция не достигнет минимума или не будет выполнено установленное кол-ао интераций.
    
- 13.  Логистическая регрессия в задачах классификации.
    
    $$
    h_{b}(x)=g(x)=\frac{1}{1+e^{-z}}
    $$
    
    Логистическая регрессия - это статистический метод, обычно используемый в машинном обучении для задач бинарной классификации, где целью является предсказание одного из двух возможных результатов, таких как истина / ложь или да / нет. Вот несколько причин, по которым логистическая регрессия широко используется в задачах классификации:
    
    **Простой и интерпретируемый**: Логистическая регрессия - это относительно простой алгоритм, который легко понять и интерпретировать. Он может дать представление о взаимосвязи между независимыми переменными и вероятностью конкретного результата.
    
    **Линейная граница принятия решения:** Логистическую регрессию можно использовать для моделирования линейных границ принятия решения, что делает ее полезной для разделения точек данных, принадлежащих разным классам.
    
    **Эффективное обучение:** Логистическую регрессию можно быстро обучить даже с большими наборами данных, и она требует меньших вычислительных затрат, чем более сложные модели, такие как нейронные сети.
    
    **Устойчивость к шуму**: Логистическая регрессия может обрабатывать шум во входных данных и менее подвержена переоснащению по сравнению с другими алгоритмами машинного обучения.
    
    **Хорошо работает с небольшими наборами данных:** Логистическая регрессия может хорошо работать даже при ограниченном количестве доступных данных, что делает ее полезным алгоритмом при работе с небольшими наборами данных.
    
    В целом, логистическая регрессия является популярным и эффективным методом для задач бинарной классификации. Однако он может не подходить для более сложных задач классификации, где существует несколько классов или нелинейные взаимосвязи между входными переменными и результатом.
    
    Логистическая регрессия является частным случаем линейного классификатора, но она обладает хорошим "умением" – прогнозировать вероятность  отнесения примера  к классу "+":
    
    $$
    p_{+}=P(y_{i} = 1 | \large\bar{x_{i}}, \bar{w})
    $$
    
    Прогнозирование не просто ответа ("+1" или "-1"), а именно *вероятности* отнесения к классу "+1" во многих задачах является очень важным бизнес-требованием.
    
    **Применение:**
    
    - **Классификация спама:** Отнесение писем к спаму или не спаму.
    - **Классификация клиентов:** Разделение клиентов на группы с высокой или низкой ценностью.
    - **Прогнозирование заболеваний:** Предсказание вероятности развития заболевания на основе медицинских показателей.
    - **Распознавание изображений:** Классификация объектов на изображениях (например, идентификация кошек и собак).
    
    Отличия линейной и логистической(для поболтать):
    
    [Открытый курс машинного обучения. Тема 4. Линейные модели классификации и регрессии / Хабр (habr.com)](https://habr.com/ru/companies/ods/articles/323890/)
    
    На входе логистическая регрессия, как и линейная, принимает одну или несколько независимых переменных (признаков набора данных) и подсчитывает их взаимосвязь с зависимой переменной. Различие в том, что логистическая регрессия применяет **сигмоидную функцию** (также известна как **логистическая**, или **логит-функция**), которая позволяет предсказывать непрерывную переменную со значениями на отрезке [0, 1] при любых значениях независимых переменных. Фактически это распределение Бернулли.
    
    **Логистическая регрессия вычисляет вероятность того, что данное исходное значение принадлежит к определенному классу. Она используется для задач классификации:** оценивает апостериорные вероятности принадлежности данного объекта к тому или иному классу.
    
- 14. Множественная и многоклассовая классификация. Алгоритм “один против всех”.
    
    [https://habr.com/ru/companies/otus/articles/766034/](https://habr.com/ru/companies/otus/articles/766034/)
    
    [https://scikit-learn.ru/1-12-multiclass-and-multioutput-algorithms/](https://scikit-learn.ru/1-12-multiclass-and-multioutput-algorithms/)
    
    Множественная классификация - это точка семитических вариаций, в которой объект может быть прямым экземпляром сразу нескольких классов. При динамической классификации объекты могут приобретать и терять классы во время выполнения. Это позволяет использовать классы как временные роли, которые может играть объект.
    
    Многоклассовая классификация - это задача [машинного обучения](https://translated.turbopages.org/proxy_u/en-ru.ru.b986c3f1-6659aa62-ff4abd23-74722d776562/https/www.geeksforgeeks.org/machine-learning/), направленная на разделение данных более чем на два класса. В то время как бинарная классификация предполагает различение только двух классов, многоклассовая классификация расширяет эту область, включая различение нескольких классов. По сути, цель состоит в обучении модели, которая может эффективно сортировать экземпляры по различным предопределенным категориям, обеспечивая детализированное решение для сценариев, в которых элементы могут принадлежать более чем двум эксклюзивным группам. Этот подход обычно используется в таких задачах, как распознавание рукописного ввода, категоризация электронной почты и классификация изображений, включающих более двух различных категорий.
    
    Многоклассовая классификация - это тип задачи машинного обучения, целью которой является отнесение экземпляров к одному из нескольких предопределенных классов. В отличие от бинарной классификации, где возможны только два результата, многоклассовая классификация предполагает различение нескольких классов или категорий. Основная идея состоит в том, чтобы научить модель присваивать наиболее подходящую метку класса каждому экземпляру на основе его функций.
    
    Рассмотрим несколько примеров задач, где она играет важную роль:
    
    1. Классификация изображений: В области компьютерного зрения, задачи классификации изображений часто являются многоклассовыми. Например, классификация фотографий животных, растений, автомобилей и других объектов.
    2. Распознавание рукописного текста: Задача определения, какой символ или буква написана на изображении, также является многоклассовой классификацией.
    3. Медицинская диагностика: В медицинской сфере многоклассовая классификация применяется для определения диагнозов на основе медицинских изображений, таких как рентгеновские снимки, снимки МРТ или УЗИ.
    4. Анализ тональности текста: Классификация текстовых обзоров или комментариев на положительные, отрицательные или нейтральные мнения - это еще один пример задачи многоклассовой классифик
    
    Один-против-остальных (one-vs-rest) стратегии, также известный как один-против-всех , реализуется [OneVsRestClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.multiclass.OneVsRestClassifier.html#sklearn.multiclass.OneVsRestClassifier). Стратегия заключается в подборе одного классификатора на класс. Для каждого классификатора класс сопоставляется со всеми другими классами. Помимо вычислительной эффективности ( n_classes нужны только классификаторы), одним из преимуществ этого подхода является его интерпретируемость. Поскольку каждый класс представлен одним и только одним классификатором, можно получить информацию о классе, проверив соответствующий классификатор.
    
    ![Untitled](%D0%9C%D0%B0%D1%88%D0%B8%D0%BD%D0%BD%D0%BE%D0%B5%20%D0%BE%D0%B1%D1%83%D1%87%D0%B5%D0%BD%D0%B8%D0%B5%20%D1%8D%D0%BA%D0%B7%D0%B0%D0%BC%D0%B5%D0%BD%2004f25e7078ad4106b73d3ba8d4abfdb3/Untitled%207.png)
    
    Чатик:
    
    Алгоритм «один против всех» (one-vs-all) — это метод, используемый в многоклассовой классификации. С его помощью можно превратить многоклассовую задачу классификации в несколько задач бинарной классификации.
    
    Применительно к методам машинного обучения, алгоритм заключается в том, что для каждого класса строится свой классификатор, при этом данный классификатор определяет, принадлежит ли объект данному классу или нет, в то время как все другие объекты объединяются во второй класс.
    
    По сути, в реализации алгоритма «один против всех» мы создаем несколько бинарных классификаторов, каждый из которых отличает один класс от всех остальных.
    
- 15.Метод опорных векторов в задачах классификации.
    
    **Метод опорных векторов SVM (Support Vector Machines)** — это алгоритм машинного обучения, используемый для решения задач классификации, регрессии и обнаружения выбросов.
    
    **Случай линейно разделимых классов** (т.е. таких, для которых возможно построить гиперплоскость в n-мерном пространстве для разделения объектов двух классов): Модель SVM ищет гиперплоскость с максимальной шириной разделяющей полосы (margin), называемой линией зазора, то есть разделяющую гиперплоскость, максимально отдаленную от обоих классов.
    
    Уравнения гиперплоскости и линий зазора имеют вид соответственно:
    
    ![Untitled](%D0%9C%D0%B0%D1%88%D0%B8%D0%BD%D0%BD%D0%BE%D0%B5%20%D0%BE%D0%B1%D1%83%D1%87%D0%B5%D0%BD%D0%B8%D0%B5%20%D1%8D%D0%BA%D0%B7%D0%B0%D0%BC%D0%B5%D0%BD%2004f25e7078ad4106b73d3ba8d4abfdb3/Untitled%208.png)
    
    Оптимальные значения 𝑤 и 𝑏 подбираются в результате решения задачи оптимизации, которая заключается в минимизации функции потерь и максимизации зазора между классами.
    
    Метод опорных векторов, решает задачу оптимизации, подбирая прямую с максимальным зазором между ближайшими точками разных классов. Эти точки называются опорными векторами.
    
    В библиотеке `Scikit-Learn` метод опорных векторов для классификации реализован в `sklearn.svm.SVC`. Для линейно разделимых данных можно использовать класс SVC с линейным ядром (`kenel = 'linear'`). Также в `Scikit-Learn` есть другая реализация метода опорных векторов для случая линейной разделимости данных `sklearn.svm.LinearSVC`, но в нём нет возможности выводить информацию об опорных векторах.
    
    Классификация методом опорных векторов может осуществляться с *жёстким зазором* (hard margin classification) и с *мягким зазором* (soft margin classification).
    
    При классификации с *жестким зазором* все образцы должны находятся вне линии зазора.
    
    При классификацией с мягким зазором допускаются нарушения границы зазора, т.е. появления экземпляров, которые оказываются внутри полосы зазора или даже на неправильной стороне, отыскивается баланс между удержанием полосы как можно более широкой и ограничением количества нарушений зазора.
    
    Классификация с жестким зазором имеет недостатки:
    
    - она работает, только если данные являются линейно разделимыми.
    - она довольно чувствительна к выбросам.
    
    В классах `SVM` библиотеки `Scikit-Learn` можно управлять упомянутым балансом, используя гиперпараметр `C`: меньшее значение `C` ведет к более широкой полосе, но большему числу нарушений зазора, а чем больше `C`, тем более жёсткий зазор.
    
    **Случай линейно неразделимых классов**:
    
    В случае линейной неразделимости вводятся дополнительные переменные, характеризующие допустимую ошибку классификации на различных объектах, и применяется трюк, заключающийся в переходе от скалярного произведения к нелинейной функции ядра (kernel trick). Данный прием позволяет перейти в пространство большей размерности, где выборка может быть линейно разделена.
    
    Три вида ядер:
    
    1. *Линейное ядро* (kernel = 'linear' ) — это ядро, которое строит гиперплоскость для разделения данных. Оно часто используется в задачах с линейно разделимыми данными. В математическом смысле линейное ядро вычисляет скалярное произведение между векторами признаков объектов. Если объекты невозможно разделить линейной гиперплоскостью, то следует с этим ядром использовать классификацию с мягким зазором (soft margin), которая позволяет допустить ошибки классификации и при этом сохранить разделяющую гиперплоскость.
    2. *Радиальное базисное ядро* (kernel = 'rbf') — это наиболее часто используемое ядро (установлено по умолчанию), которое может разделять данные, не являющиеся линейно разделимыми. Оно создает границу принятия решений в виде радиально-симметричного колокола.
    3. *Ядро с полиномиальной функцией* (kernel = 'poly') — это ядро, которое вводит полиномиальную функцию в пространство признаков для разделения данных. Это может быть полезно для данных, которые не могут быть разделены гиперплоскостью.
- 16.Понятие ядра и виды ядер в методе опорных векторов.
    
    В случае линейной неразделимости вводятся дополнительные переменные, характеризующие допустимую ошибку классификации на различных объектах, и применяется трюк, заключающийся в переходе от скалярного произведения к нелинейной функции ядра (kernel trick). Данный прием позволяет перейти в пространство большей размерности, где выборка может быть линейно разделена.
    
    Три вида ядер:
    
    1. *Линейное ядро* (kernel = 'linear' ) — это ядро, которое строит гиперплоскость для разделения данных. Оно часто используется в задачах с линейно разделимыми данными. В математическом смысле линейное ядро вычисляет скалярное произведение между векторами признаков объектов. Если объекты невозможно разделить линейной гиперплоскостью, то следует с этим ядром использовать классификацию с мягким зазором (soft margin), которая позволяет допустить ошибки классификации и при этом сохранить разделяющую гиперплоскость.
    2. *Радиальное базисное ядро* (kernel = 'rbf') — это наиболее часто используемое ядро (установлено по умолчанию), которое может разделять данные, не являющиеся линейно разделимыми. Оно создает границу принятия решений в виде радиально-симметричного колокола.
    3. *Ядро с полиномиальной функцией* (kernel = 'poly') — это ядро, которое вводит полиномиальную функцию в пространство признаков для разделения данных. Это может быть полезно для данных, которые не могут быть разделены гиперплоскостью.
- 17. Метод решающих деревьев в задачах классификации.
    
    **Метод решающих деревьев** (Decision Trees) — это алгоритм машинного обучения, используемый для решения задач классификации и регрессии. В задачах классификации решающее дерево разбивает пространство признаков на области, соответствующие различным классам, и создает модель в виде дерева, где каждое внутреннее узло представляет собой условие по одному из признаков, а каждый лист — окончательное решение или класс.
    
    **Основные компоненты решающих деревьев**
    
    *1. Корневой узел:* Начальная точка дерева, содержащая весь набор данных.
    
    *2. Внутренние узлы:* Узлы, представляющие условия (признаки и их значения) для разделения данных.
    
    *3. Листовые узлы:* Узлы, представляющие окончательное решение или класс.
    
    *4. Ветви:* Пути, исходящие из узлов и ведущие к другим узлам или листам, представляющие результат условия.
    
    **Принцип работы решающих деревьев**
    
    *1. Выбор признака для разделения:* На каждом этапе алгоритм выбирает признак, который наилучшим образом разделяет данные. Критерии выбора включают:
    
    *- Информационная выгода:* Измеряет уменьшение неопределенности или энтропии после разделения данных.
    
    *- Критерий Джини*: Измеряет вероятность неправильной классификации объекта, выбранного случайным образом.
    
    *- Чистота узлов*: Стремление к тому, чтобы узлы содержали данные одного класса.
    
    *2. Разделение данных:* Данные разделяются на подмножества на основе выбранного признака и его значений.
    
    *3. Рекурсивное построение:* Процесс выбора признака и разделения данных повторяется рекурсивно для каждого подмножества, создавая внутренние узлы и ветви до тех пор, пока не будет достигнут критерий останова (например, максимальная глубина дерева или минимальное количество образцов в узле).
    
    *4. Присвоение классов*: Когда невозможно больше разделять данные, узлы становятся листами, и каждому листу присваивается класс, который является наиболее распространенным среди образцов в этом узле.
    
    **Пример построения решающего дерева**
    
    Рассмотрим задачу классификации с набором данных о цветках ириса (Iris dataset), где необходимо классифицировать цветки по трем видам (Setosa, Versicolor, Virginica) на основе длины и ширины лепестков и чашелистиков.
    
    *1. Выбор признака и порога:* Допустим, что лучший признак для разделения на первом шаге — это длина лепестка. Алгоритм выбирает порог, который наилучшим образом разделяет данные, например, длина лепестка < 2.5 см.
    
    *2. Создание узлов и ветвей:* Данные делятся на две группы:
    
    - Длина лепестка < 2.5 см (это будут цветки Setosa).
    
    - Длина лепестка >= 2.5 см (эти данные будут разделяться дальше).
    
    *3. Рекурсивное построение:* Для каждой группы процесс повторяется. Например, для группы с длиной лепестка >= 2.5 см следующий лучший признак — это ширина лепестка.
    
    *4. Завершение дерева:* Процесс продолжается, пока не будут достигнуты критерии останова.
    
- 18. Метод k ближайших соседей в задачах классификации.
    
    **Метод k-ближайших соседей** (k-NN) — это простой и эффективный алгоритм машинного обучения, используемый для задач классификации и регрессии. В задачах классификации метод k-NN присваивает метку класса для нового наблюдения на основе классов его k ближайших соседей в пространстве признаков.
    
    **Основные компоненты метода k-NN**
    
    *1. Пространство признаков:* Представление данных в виде точек в многомерном пространстве, где каждая точка соответствует одному наблюдению с определённым набором признаков.
    
    *2. Метрика расстояния:* Функция, измеряющая расстояние между точками в пространстве признаков. Наиболее распространённые метрики включают:
    
    - Евклидово расстояние - расстояние между двумя точками в пространстве, которое можно определить с помощью теоремы Пифагора 
    
    - Манхэттенское расстояние - расстояние между двумя точками, измеренное вдоль осей, расположенных под прямым углом друг к другу (городские кварталы)
    
    - Расстояние Минковского - метрика в нормированном векторном пространстве, которую можно рассматривать как обобщение как евклидова расстояния, так и манхэттенского расстояния 
    
    *3. Число соседей (k):* Количество ближайших соседей, которые учитываются при классификации нового наблюдения.
    
    **Принцип работы метода k-NN**
    
    *1. Вычисление расстояний*: Для нового наблюдения вычисляются расстояния до всех точек в обучающем наборе данных.
    
    *2. Определение ближайших соседей:* Выбираются k ближайших точек (соседей) на основе выбранной метрики расстояния.
    
    *3. Классификация:* Новый объект классифицируется путём голосования: присваивается класс, который наиболее часто встречается среди k ближайших соседей.
    
- 19. Однослойный перцептрон в задачах классификации.
    
    Однослойный перцептрон (SLP) — **одна из простейших форм искусственных нейронных сетей.**
    
    **Архитектура однослойного перцептрона**
    
    Однослойный перцептрон состоит из одного слоя нейронов, где каждый нейрон подключен ко всем входным признакам. Перцептрон принимает входные данные, умножает их на веса, суммирует и передает через активационную функцию (обычно это пороговая функция).
    
    **Математическое описание**
    
    Модель однослойного перцептрона можно описать следующим образом:
    
    1. Входные данные: x=[x1,x2,…,xn]
    2. Веса: w=[w1,w2,…,wn]
    3. Смещение (bias): b
    
    Выходное значение нейрона вычисляется как:
    
    y={1, если $\sum_{i=1}^n w_{i}* x_{i} > 0$
    
        {0, иначе
    
    **Обучение перцептрона**
    
    Обучение однослойного перцептрона происходит путем корректировки весов с использованием правила обновления весов:
    
    1. Инициализация весов случайным образом.
    2. Для каждого обучающего примера (x,t), где t — целевой класс:
        - Вычисление предсказания y.
        - Обновление весов и смещения:
        $w_{i}$←$w_{i}$+η(t−y)$x_{i}$
            
            b←b+η(t−y)
            
        
        где η — скорость обучения.
        
- 20. Метрики эффективности и функции ошибки: назначение, примеры, различия.
    
    Метрика – это способ измерения того, насколько хорошо работает ваша модель
    
    **Примеры метрик для задач регрессии:**
    
    1. Среднеквадратическая ошибка (Mean Squared Error, MSE):
        
        ![Untitled](%D0%9C%D0%B0%D1%88%D0%B8%D0%BD%D0%BD%D0%BE%D0%B5%20%D0%BE%D0%B1%D1%83%D1%87%D0%B5%D0%BD%D0%B8%D0%B5%20%D1%8D%D0%BA%D0%B7%D0%B0%D0%BC%D0%B5%D0%BD%2004f25e7078ad4106b73d3ba8d4abfdb3/Untitled%209.png)
        
    2. Средняя абсолютная ошибка (Mean Absolute Error, MAE):
        
        ![Untitled](%D0%9C%D0%B0%D1%88%D0%B8%D0%BD%D0%BD%D0%BE%D0%B5%20%D0%BE%D0%B1%D1%83%D1%87%D0%B5%D0%BD%D0%B8%D0%B5%20%D1%8D%D0%BA%D0%B7%D0%B0%D0%BC%D0%B5%D0%BD%2004f25e7078ad4106b73d3ba8d4abfdb3/Untitled%2010.png)
        
    3. Коэффициент детерминации (R^2):
        
        ![Untitled](%D0%9C%D0%B0%D1%88%D0%B8%D0%BD%D0%BD%D0%BE%D0%B5%20%D0%BE%D0%B1%D1%83%D1%87%D0%B5%D0%BD%D0%B8%D0%B5%20%D1%8D%D0%BA%D0%B7%D0%B0%D0%BC%D0%B5%D0%BD%2004f25e7078ad4106b73d3ba8d4abfdb3/Untitled%2011.png)
        
        где yˉ — среднее значение y.
        
    4. RMSE (корень квадратный из MSE)
    
     MSE и RMSE чувствительнее к выбросам, чем MAE. Это связано с формулой расчёта — возведение в квадрат сильно увеличивает итоговые значения метрики модели, особенно из-за выбросов. При взятии модуля такого не происходит.
    
    Метрика MAE более робастная, поэтому её используют, когда влияние аномальных данных на предсказание модели нужно свести к минимуму. Поэтому:
    
    - Метрики MSE/RMSE подойдут, если у целевого признака высокая дисперсия, и эту особенность данных нужно учесть. MAE даст слишком оптимистичную оценку качества на таких данных.
    - MAE будет точнее отражать качество модели, если слишком большие или слишком малые значения — это выбросы, которыми можно пренебречь.
    
    ****Робастный*** — устойчивый к «помехам», то есть к выбросам в данных.
    
    **Примеры метрик для задач классификации:**
    
    1. Точность (Accuracy):
        
        ![Untitled](%D0%9C%D0%B0%D1%88%D0%B8%D0%BD%D0%BD%D0%BE%D0%B5%20%D0%BE%D0%B1%D1%83%D1%87%D0%B5%D0%BD%D0%B8%D0%B5%20%D1%8D%D0%BA%D0%B7%D0%B0%D0%BC%D0%B5%D0%BD%2004f25e7078ad4106b73d3ba8d4abfdb3/Untitled%2012.png)
        
    2. Полнота (Recall):
        
        ![Untitled](%D0%9C%D0%B0%D1%88%D0%B8%D0%BD%D0%BD%D0%BE%D0%B5%20%D0%BE%D0%B1%D1%83%D1%87%D0%B5%D0%BD%D0%B8%D0%B5%20%D1%8D%D0%BA%D0%B7%D0%B0%D0%BC%D0%B5%D0%BD%2004f25e7078ad4106b73d3ba8d4abfdb3/Untitled%2013.png)
        
        ![Untitled](%D0%9C%D0%B0%D1%88%D0%B8%D0%BD%D0%BD%D0%BE%D0%B5%20%D0%BE%D0%B1%D1%83%D1%87%D0%B5%D0%BD%D0%B8%D0%B5%20%D1%8D%D0%BA%D0%B7%D0%B0%D0%BC%D0%B5%D0%BD%2004f25e7078ad4106b73d3ba8d4abfdb3/Untitled%2014.png)
        
        Recall измеряет, смогла ли модель классификации присвоить класс 1 всем объектам этого класса
        
    3. Точность (Precision):
        
        ![Untitled](%D0%9C%D0%B0%D1%88%D0%B8%D0%BD%D0%BD%D0%BE%D0%B5%20%D0%BE%D0%B1%D1%83%D1%87%D0%B5%D0%BD%D0%B8%D0%B5%20%D1%8D%D0%BA%D0%B7%D0%B0%D0%BC%D0%B5%D0%BD%2004f25e7078ad4106b73d3ba8d4abfdb3/Untitled%2015.png)
        
        **precision** определяет, не слишком ли часто модель выставляет класс 1 объектам класса 0. Чем выше эта метрика, тем меньше таких случаев
        
    4. F1-мерa:
        
        ![Untitled](%D0%9C%D0%B0%D1%88%D0%B8%D0%BD%D0%BD%D0%BE%D0%B5%20%D0%BE%D0%B1%D1%83%D1%87%D0%B5%D0%BD%D0%B8%D0%B5%20%D1%8D%D0%BA%D0%B7%D0%B0%D0%BC%D0%B5%D0%BD%2004f25e7078ad4106b73d3ba8d4abfdb3/Untitled%2016.png)
        
        F1-мера - среднее гармоническое recall и precision
        
    5. ROC-AUC:
    Площадь под кривой ошибок (Receiver Operating Characteristic Curve), которая отображает отношение между полнотой и долей ложных положительных срабатываний.
- 21. Понятие набора данных (датасета) в машинном обучении. Требования, представление. Признаки и объекты.
    
     **Датасет** - это набор данных, используемый для обучения моделей. Данные представлены в виде единой таблицы. Набор данных состоит из объектов, каждый из которых описывается набором признаков.
    
    Объекты - это элементарные сущности, которые мы изучаем, объекты реального мира, измерения, наблюдения. В датасете представляются строками.
    
    Каждая таблица, файл представляет собой данные об одном виде наблюдений или экспериментов и должна иметь внутреннюю согласованность. 
    
    **Дополнительно:** Все данные должны быть выражены в численном виде, в данных не должно быть отсутствующих (пропущенных) значений.
    
- 22. Шкалы измерения признаков. Виды шкал, их характеристика.
    
    Шкалы измерения — это классификация способов, с помощью которых могут быть измерены и выражены характеристики или признаки объектов.
    
    ![Untitled](%D0%9C%D0%B0%D1%88%D0%B8%D0%BD%D0%BD%D0%BE%D0%B5%20%D0%BE%D0%B1%D1%83%D1%87%D0%B5%D0%BD%D0%B8%D0%B5%20%D1%8D%D0%BA%D0%B7%D0%B0%D0%BC%D0%B5%D0%BD%2004f25e7078ad4106b73d3ba8d4abfdb3/Untitled%2017.png)
    
    **Описание шкалы измерения атрибутов:**
    
    1. Шкалы атрибутов показывают, какие значения может принимать этот атрибут и как его можно
    интерпретировать и сравнивать.
    2. Все атрибуты подразделяются на численные (непрерывные) и категориальные (дискретные).
    3. Категориальные переменные потом придется преобразовать в численные.
    4. Номинальная шкала - это признак, для которого имеет смысл только равенство. Пример - метка
    класса.
    5. Порядковая шкала - это когда наряду с равенством мы можем выстроить значения по порядку,
    который имеет смысл. Пример - класс обслуживания.
    6. Интервальная шкала - это когда еще имеет смЫсл говорить о разнице между двумя значениями.
    Пример - даты.
    7. Абсолютная шкала - это когда еще можно говорить о том, во сколько раз одно значение больше
    другого. Пример - сумма денег.
    8. Особенные типы шкал - бинарная и логарифмическая.
    9. Кроме типа шкала характеризуется диапазоном или набором значений, мерой центрального
    элемента и разброса, распределением.
    10. Самый полезный инструмент для характеристики шкалы - гистограммы.
    
- 23. Понятие чистых данных. Определение, очистка данных.
    
    Чистые данные:
    
    1. Датасет - это набор данных, используемый для обучения моделей. Данные представлены в виде единой таблицы.
    2. Объекты - это элементарные сущности, которые мы изучаем, объекты реального мира, измерения, наблюдения. В датасете представляются строками.
    3. Каждый объект характеризуется набором атрибутов. В датасете атрибуты представляются столбцами.
    4. Каждая таблица, файл представляет собой данные об одном виде наблюдений или экспериментов и должна иметь внутреннюю согласованность.
    5. Дополнительно: Все данные должны быть выражены в численном виде.
    6. Дополнительно: В данных не должно быть отсутствующих (пропущенных) значений.
    7. Существуют специальные структуры данных и форматы.
    
    Шаги в очистке данных:
    
    1. **Удаление дубликатов**
    2. **Заполнение пропусков**
    3. **Исправление ошибок**
    4. **Удаление данных не относящихся к делу**
    5. **Проверка на непротиворечивость**
    
- 24. Основные этапы проекта по машинному обучению.
    - **Определение проблемы и постановка задачи**
        - **Цель**: Понять и четко определить проблему, которую нужно решить с помощью машинного обучения.
        - **Пример**: Улучшение точности прогнозирования спроса на продукцию.
    - **Сбор данных**
        - **Цель**: Сбор всех необходимых данных из различных источников.
        - **Пример**: Собрать исторические данные о продажах, маркетинговых кампаниях, сезонных колебаниях и т.д.
    - **Очистка и подготовка данных**
        - **Цель**: Обработка данных для удаления ошибок, дубликатов, пропущенных значений и приведение данных к единому формату.
        - **Пример**: Заполнение пропущенных значений, удаление дубликатов записей, нормализация данных.
    - **Анализ данных и исследовательский анализ данных (EDA)**
        - **Цель**: Изучение данных для понимания их структуры, выявления закономерностей и аномалий.
        - **Пример**: Визуализация данных с помощью графиков, расчет основных статистик, поиск корреляций.
    - **Выбор признаков (Feature Engineering)**
        - **Цель**: Создание и выбор наиболее значимых признаков (фич), которые будут использоваться для обучения модели.
        - **Пример**: Создание новых признаков на основе существующих данных, выбор наиболее значимых признаков с помощью методов отбора признаков.
    - **Выбор модели и алгоритма**
        - **Цель**: Определение подходящей модели машинного обучения и алгоритма для решения поставленной задачи.
        - **Пример**: Выбор между линейной регрессией, решающими деревьями, нейронными сетями и другими алгоритмами.
    - **Обучение модели**
        - **Цель**: Настройка и обучение модели на тренировочных данных.
        - **Пример**: Обучение модели с использованием тренировочных данных и настройка гиперпараметров для улучшения производительности.
    - **Оценка модели**
        - **Цель**: Оценка качества модели с помощью тестовых данных и метрик производительности.
        - **Пример**: Оценка модели с использованием метрик, таких как точность (accuracy), полнота (recall), F1-мера.
    - **Тестирование и валидация**
        - **Цель**: Проведение кросс-валидации и тестирования модели на новых данных для проверки ее надежности.
        - **Пример**: Проведение кросс-валидации, тестирование на отложенной выборке.
    - **Внедрение модели**
        - **Цель**: Внедрение модели в рабочую среду и интеграция с существующими системами.
        - **Пример**: Деплой модели на сервере, интеграция с веб-приложением для использования пользователями.
    - **Мониторинг и обновление**
        - **Цель**: Постоянный мониторинг производительности модели и обновление ее по мере необходимости.
        - **Пример**: Отслеживание точности прогнозов, обновление модели при изменении данных или условий задачи.
        
- 25. Предварительный анализ данных: задачи, методы, цели.
    
    Предварительный анализ данных (Exploratory Data Analysis, EDA) является важной частью любого проекта, связанного с данными. Его целью является изучение, анализ и визуализация данных для лучшего понимания их структуры, особенностей, закономерностей и аномалий. Предварительный анализ данных позволяет выявить проблемы с данными, определить направления дальнейшей работы и сделать выводы, которые могут быть использованы для принятия решений.
    
    Задачи предварительного анализа данных:
    
    1. Ознакомление с данными: изучение структуры данных, их типов, размеров, форматов и источников.
    2. Статистический анализ: вычисление основных статистических показателей, таких как среднее значение, медиана, мода, дисперсия, стандартное отклонение, коэффициент вариации и других.
    3. Визуализация данных: создание графиков, диаграмм и других визуальных представлений данных для лучшего понимания их структуры, распределения и взаимосвязей.
    4. Обработка пропусков и аномалий: выявление и обработка пропусков и аномалий в данных, таких как отсутствующие значения, выбросы и некорректные данные.
    5. Преобразование данных: преобразование данных для лучшего соответствия требованиям модели машинного обучения, таких как нормализация, стандартизация, бинарное кодирование и другие.
    6. Выделение признаков: выделение наиболее важных признаков для модели машинного обучения, таких как выбор признаков, снижение размерности и другие.
    
    Методы предварительного анализа данных:
    
    1. Описательная статистика: вычисление среднего значения, медианы, моды, дисперсии, стандартного отклонения и других статистических показателей.
    2. Графический анализ: создание гистограмм, бокс-плотов, диаграмм рассеивания и других графиков для визуализации данных.
    3. Корреляционный анализ: выявление взаимосвязей между признаками и целевой переменной с помощью коэффициента корреляции Пирсона или Спирмена.
    4. Анализ главных компонент: снижение размерности путем преобразования исходных признаков в новые ортогональные признаки, соответствующие главным компонентам.
    5. Кластерный анализ: объединение похожих объектов в кластеры на основе их признаков.
    
    Цели предварительного анализа данных:
    
    1. Лучшее понимание данных: изучение структуры, особенностей, закономерностей и аномалий данных.
    2. Выявление проблем с данными: обнаружение пропусков, выбросов, некорректных данных и других проблем, которые могут повлиять на качество модели машинного обучения.
    3. Определение направлений дальнейшей работы: выявление признаков, которые могут быть использованы для моделирования, и определение подходов к преобразованию и моделированию данных.
    4. Принятие решений на основе данных: выводы, сделанные на основе предварительного анализа данных, могут быть использованы для принятия решений в бизнесе, науке и других сферах.
- 26.Проблема отсутствующих данных: причины, исследование, пути решения.
    
    **Ответ:**
    
    Проблема отсутствующих данных является одной из наиболее распространенных и сложных проблем при анализе данных. Отсутствие данных может быть вызвано различными причинами, такими как ошибки при сборе данных, потеря данных при передаче или хранении, нежелание или невозможность предоставить информацию и т.д.
    
    Исследование проблемы отсутствующих данных включает в себя оценку степени и характера отсутствующих данных, а также выбор подходящего метода для их обработки.
    
    Существует несколько подходов к классификации отсутствующих данных:
    
    1. По степени отсутствия:
    - Отсутствуют случайным образом (MCAR, Missing Completely at Random): отсутствие данных не зависит от значений других переменных или от самих отсутствующих значений.
    - Отсутствуют в зависимости от наблюдаемых данных (MAR, Missing at Random): отсутствие данных зависит от значений других переменных, но не зависит от самих отсутствующих значений.
    - Отсутствуют не случайным образом (NMCAR, Not Missing at Random): отсутствие данных зависит от самих отсутствующих значений.
    1. По типу отсутствующих данных:
    - Отсутствуют полностью (компонента, переменная, объект): все значения для данной компоненты, переменной или объекта отсутствуют.
    - Отсутствуют частично: некоторые значения для данной компоненты, переменной или объекта отсутствуют.
    
    Пути решения проблемы отсутствующих данных зависят от степени и характера отсутствующих данных, а также от целей анализа. Некоторые из наиболее распространенных методов обработки отсутствующих данных включают в себя:
    
    - Удаление объектов или переменных с отсутствующими значениями. Этот метод может быть эффективным при небольшом количестве отсутствующих данных, но может привести к потере важной информации и искажению результатов анализа при большом количестве отсутствующих данных.
    - Замена отсутствующих значений средним, медианным или модальным значением для данной переменной. Этот метод может быть эффективным при отсутствии данных случайным образом, но может привести к искажению результатов анализа при отсутствии данных в зависимости от наблюдаемых данных или не случайным образом.
    - Использование методов машинного обучения для предсказания отсутствующих значений на основе значений других переменных. Этот метод может быть эффективным при отсутствии данных в зависимости от наблюдаемых данных, но требует наличия достаточного количества данных для обучения модели и может привести к искажению результатов анализа при отсутствии данных не случайным образом.
    - Использование методов статистического моделирования, таких как регрессия, факторный анализ или кластерный анализ, для оценки отсутствующих значений на основе значений других переменных. Этот метод может быть эффективным при отсутствии данных в зависимости от наблюдаемых данных или не случайным образом, но требует наличия достаточного количества данных для моделирования и может быть сложным для реализации.
    
- 27.  Проблема несбалансированных классов: исследование, пути решения.
    
    **Исследование проблемы несбалансированных классов**
    
    В задачах классификации проблема несбалансированных классов возникает, когда количество образцов одного класса значительно отличается от количества образцов другого класса. Это приводит к тому, что модель может быть склонна к предсказанию более частого класса и игнорированию менее представленного класса. Несбалансированные классы часто встречаются в таких приложениях, как обнаружение мошенничества, медицинская диагностика и обнаружение спама.
    
    **Примеры**
    
    *- Обнаружение мошенничества*: Только небольшой процент транзакций является мошенническим.
    
    *- Медицинская диагностика:* Определённое заболевание может быть редким по сравнению с общим здоровым населением.
    
    *- Обнаружение спама:* Большинство электронных писем не являются спамом, и только небольшая часть помечается как спам.
    
    **Влияние на модели машинного обучения**
    
    *- Смещение предсказаний:* Модель может обучаться игнорировать меньшинство и просто предсказывать большинство, что ведёт к высокой общей точности, но низкой чувствительности к меньшинству.
    
    *- Неправильная оценка производительности:* Метрики, такие как точность (accuracy), могут вводить в заблуждение при оценке производительности модели на несбалансированных данных.
    
    **Пути решения проблемы несбалансированных классов**
    
    Для борьбы с проблемой несбалансированных классов существует несколько подходов, которые можно разделить на методы обработки данных и методы алгоритмов.
    
    *a.Методы обработки данных*
    
    *1. Ресэмплирование*
    
    *- Oversampling (увеличение выборки меньшинства):* Генерация дополнительных образцов меньшинства.
    
    *- Undersampling (уменьшение выборки большинства):* Удаление некоторых образцов большинства для уменьшения дисбаланса.
    
    *2. Синтетические данные*
    
    *- SMOTE:* Создание синтетических образцов меньшинства путём интерполяции между существующими образцами.
    
    *- ADASYN:* Подобно SMOTE, но фокусируется на более трудных для классификации образцах меньшинства.
    
    *3. Взвешивание классов*
    
    - Присвоение большего веса менее представленным классам в функции потерь, чтобы штрафовать модель сильнее за ошибки на образцах меньшинства.
    
    *b.Методы алгоритмов*
    
    *1. Ансамблевые методы*
    
    *- Bagging:* Метод Bootstrap Aggregating, который можно применять с ресэмплированием для создания сбалансированных подвыборок.
    
    *- Boosting:* Адаптивные методы, такие как AdaBoost или Gradient Boosting, которые могут уделять больше внимания менее представленным классам.
    
    *2. Специализированные алгоритмы*
    
    - *Balanced Random Forest:* Модификация случайного леса, которая использует undersampling для каждой итерации.
    
    *- EasyEnsemble и BalanceCascade:* Алгоритмы, специально разработанные для работы с несбалансированными данными.
    
- 28. Понятие параметров и гиперпараметров модели. Обучение параметров и гиперпараметров. Поиск по сетке.
    
    **Гиперпараметры модели** — это параметры, значения которых задаются до начала обучения модели и не изменяются в процессе обучения. У модели может не быть гиперпараметров.
    
    **Параметры модели** — это параметры, которые изменяются и оптимизируются в процессе обучения модели. Итоговые значения этих параметров являются результатом обучения модели.
    
    Обучение параметров модели происходит путем минимизации функции потерь (loss function) с помощью методов оптимизации, таких как градиентный спуск. Модель обновляет свои параметры на каждой итерации обучения, чтобы улучшить свои предсказания.
    
    Обучение гиперпараметров модели — это процесс выбора оптимальных значений гиперпараметров для достижения лучшей производительности модели. Этот процесс может быть выполнен с использованием методов оптимизации, таких как поиск по сетке (grid search) или случайный поиск (random search).
    
    Поиск по сетке (Grid Search) — это метод систематического перебора заданных значений гиперпараметров для модели с целью найти комбинацию, которая обеспечивает наилучшую производительность модели. При поиске по сетке задается сетка возможных значений гиперпараметров, и для каждой комбинации значений выполняется обучение и оценка модели. После завершения процесса выбирается комбинация гиперпараметров, давшая наилучший результат.
    
- 29. Понятие недо- и переобучения. Определение, пути решения.
    
    **Недообучение (Underfitting)**
    
    *Определение:*
    
    Недообучение происходит, когда модель слишком проста для описания зависимости в данных. Она не способна уловить сложные структуры и закономерности, что приводит к низкой точности как на обучающей, так и на тестовой выборке.
    
    *Признаки:*
    
    - Высокая ошибка на обучающей выборке.
    - Высокая ошибка на тестовой выборке.
    
    *Причины:*
    
    - Модель слишком простая (например, линейная модель для сильно нелинейных данных).
    - Недостаточное количество признаков.
    - Переобработка данных (например, чрезмерная нормализация).
    
    *Пути решения:*
    
    1. Усложнение модели: Использование более сложных моделей (например, переход от линейной регрессии к полиномиальной регрессии или использованию нейронных сетей).
    2. Добавление признаков: Включение дополнительных признаков, которые могут быть полезны для предсказания.
    3. Увеличение времени обучения: Продолжительное обучение может помочь модели лучше улавливать закономерности в данных.
    4. Понижение уровня регуляризации: Снижение параметров регуляризации, таких как λ в L1 или L2 регуляризации, чтобы позволить модели лучше подгоняться под данные.
    
    **Переобучение (Overfitting)**
    
    *Определение:*
    
    Переобучение происходит, когда модель слишком сложная и слишком хорошо подгоняется под обучающие данные, включая шум и незначительные вариации. Это приводит к высокой точности на обучающей выборке, но низкой точности на новых данных.
    
    *Признаки:*
    
    - Низкая ошибка на обучающей выборке.
    - Высокая ошибка на тестовой выборке.
    
    *Причины:*
    
    - Модель слишком сложная (например, глубокая нейронная сеть для небольшой выборки данных).
    - Недостаточное количество данных.
    - Чрезмерное количество признаков, которые могут включать шумовые данные.
    
    *Пути решения:*
    
    1. Упрощение модели: Использование более простых моделей (например, уменьшение числа слоев или нейронов в нейронной сети).
    2. Регуляризация: Применение методов регуляризации, таких как L1 (Lasso), L2 (Ridge) или Dropout для нейронных сетей.
    3. Кросс-валидация: Использование методов кросс-валидации для оценки обобщающей способности модели.
    4. Сбор дополнительных данных: Увеличение объема данных, чтобы модель могла лучше обобщать.
    5. Ранний останов (Early Stopping): Прекращение обучения, когда ошибка на валидационной выборке начинает увеличиваться.
    6. Снижение количества признаков: Удаление нерелевантных или шумовых признаков, используя методы отбора признаков (feature selection).
- 30. Диагностика модели машинного обучения. Методы, цели.
    
    **Цели диагностики модели**
    
    1. Оценка производительности: Определение, насколько хорошо модель справляется с задачей предсказания.
    2. Обнаружение и устранение проблем: Выявление и исправление проблем, таких как недообучение, переобучение и несбалансированные данные.
    3. Оптимизация гиперпараметров: Поиск наилучших значений гиперпараметров для улучшения производительности модели.
    4. Интерпретация модели: Понимание того, как модель принимает решения и какие признаки наиболее значимы.
    
    **Методы диагностики модели**
    
    *1. Разделение данных*
    
    - Обучающая, валидационная и тестовая выборки: Разделение данных на три части помогает оценить, как модель обобщает на невидимых данных и предотвращает переобучение.
    
    *2. Кросс-валидация*
    
    - Метод, при котором данные делятся на несколько частей, и модель обучается и тестируется несколько раз на различных комбинациях частей данных. Один из популярных методов — k-fold кросс-валидация.
    
    *3. Метрики оценки*
    
    - Метрики для классификации: Точность, полнота, точность, F1-мерa, ROC-AUC и другие.
    - Метрики для регрессии: Среднеквадратическая ошибка (MSE), средняя абсолютная ошибка (MAE), коэффициент детерминации (R²).
    
    4. *Визуализация*
    
    - Матрица ошибок (Confusion Matrix): Визуальное представление истинных и предсказанных классов.
    - ROC-кривая: График, показывающий соотношение между полнотой и долей ложных положительных срабатываний.
    - График важности признаков: Оценка значимости каждого признака для модели.
- 31. Проблема выбора модели машинного обучения. Сравнение моделей.
    
    Выбор модели машинного обучения является важным этапом в процессе разработки алгоритма. При выборе модели необходимо учитывать ряд факторов, таких как тип задачи (классификация, регрессия, кластеризация и т.д.), объем и характеристики данных, интерпретируемость модели, скорость обучения и предсказания, а также другие особенности конкретной задачи.
    
    Для сравнения моделей машинного обучения можно использовать следующие методы:
    
    1. Перекрестная проверка (Cross-Validation): Этот метод позволяет оценить производительность модели на различных подмножествах данных. Наиболее распространенные методы перекрестной проверки включают K-fold cross-validation и Stratified K-fold cross-validation.
    
    2. Метрики оценки качества модели: Использование различных метрик, таких как точность (accuracy), полнота (recall), F1-мера, коэффициент корреляции Мэтьюса (Matthews correlation coefficient) и другие, для оценки производительности моделей на тестовых данных.
    
    3. Графики и визуализация: Визуализация результатов, таких как ROC-кривая (Receiver Operating Characteristic curve) для задач классификации или кривая обучения (learning curve) для оценки процесса обучения модели.
    
    4. Сравнение времени работы и ресурсов: Оценка скорости обучения и предсказания моделей, а также требуемых ресурсов (память, вычислительная мощность).
    
    5. Использование библиотек для автоматизации сравнения: Некоторые библиотеки машинного обучения, такие как scikit-learn в Python, предоставляют инструменты для автоматического сравнения нескольких моделей и выбора лучшей на основе заданных критериев.
    
- 32. Измерение эффективности работы моделей машинного обучения. Метрики эффективности.
    
    ![Untitled](%D0%9C%D0%B0%D1%88%D0%B8%D0%BD%D0%BD%D0%BE%D0%B5%20%D0%BE%D0%B1%D1%83%D1%87%D0%B5%D0%BD%D0%B8%D0%B5%20%D1%8D%D0%BA%D0%B7%D0%B0%D0%BC%D0%B5%D0%BD%2004f25e7078ad4106b73d3ba8d4abfdb3/Untitled%2018.png)
    
    **Что такое метрики эффективности?**
    
    1. Метрики эффективности - это способ показать, насколько точно модель отражает реальный мир.
    2. Метрики эффективности должны выбираться исходя из задачи, которую решает модель.
    3. Функция ошибки и метрика эффективности - это разные вещи, к ним предъявляются разные требования.
    4. В задаче можно (и, зачастую, нужно) применять несколько метрик эффективности.
    5. Наряду с метриками эффективности есть и другие характеристики моделей - скорость обучения, скорость работы, надежность, робастность, интерпретируемость.
    6. Метрики эффективности вычисляются как правило из двух векторов - предсказанных (теоретических) значений целевой переменной и эмпирических (реальных) значений.
    7. Обычно метрики устроены таким образом, что чем выше значение, тем модель лучше.
- 33. Метрики эффективности моделей классификации. Виды, характеристика, выбор.
    
    Метрика **Accuracy** - это метрика оценки качества модели в задачах классификации, которая измеряет долю правильно классифицированных объектов относительно общего числа объектов.
    
    Когда используется Accuracy:
    Accuracy часто используется в задачах классификации, когда все классы равноправны и одинаково важны, и нет явного дисбаланса классов.
    
    Достоинства Accuracy:
    1. Простота интерпретации: Accuracy легко интерпретировать и понять - это просто доля правильно классифицированных объектов.
    2. Широкое применение: Accuracy может быть использована в различных задачах классификации и сравниваться между разными моделями.
    
    Недостатки Accuracy:
    1. Неустойчивость к дисбалансу классов: Если в данных присутствует дисбаланс классов (когда один класс встречается гораздо чаще другого), то Accuracy может быть искажен и не является информативной метрикой.
    2. Не учитывает ошибки разных типов: Accuracy не различает разные типы ошибок (ложно-положительные и ложно-отрицательные), что может быть проблематично в случаях, когда разные типы ошибок имеют разную стоимость или важность.
    
    ---
    
    Метрика **Precision** - это метрика оценки качества модели в задачах классификации, которая измеряет долю правильно предсказанных положительных классов относительно всех объектов, которые модель предсказала как положительные.
    
    Когда используется Precision:
    Precision часто используется в задачах классификации, когда важно измерить точность модели в предсказании положительных классов. Она особенно полезна в ситуациях, когда ложно-положительные результаты имеют высокую стоимость или нежелательны. Например, обнаружение спама в электронной почте, где ложное положительное значение означает, что письмо, не являющееся спамом, было идентифицировано как спам. Пользователь может потерять важные письма, если точность (precision) модели обнаружения спама будет невысокой.
    
    Достоинства Precision:
    
    Учитывает ложно-положительные результаты: Precision фокусируется на правильных предсказаниях положительного класса и помогает избежать ложно-положительных результатов.
    
    Недостатки Precision:
    
    Не учитывает ложно-отрицательные результаты: Precision не учитывает ложно-отрицательные результаты, что может привести к недооценке качества модели в случае, если они также имеют высокую стоимость.
    
    ---
    
    Метрика **Recall** (полнота) - это метрика оценки качества модели в задачах классификации, которая измеряет долю правильно предсказанных положительных классов относительно всех истинных положительных классов.
    
    Когда используется Recall:
    Recall часто используется в задачах классификации, когда важно оценить способность модели обнаруживать все положительные случаи, минимизируя количество ложно-отрицательных результатов. Например, в модели обнаружения мошенничества, если мошенническая транзакция предсказывается как не мошенническая (ложно-отрицательный), последствия могут быть очень плохими для банка.
    
    Достоинства Recall:
    
    Учитывает ложно-отрицательные результаты: Recall фокусируется на способности модели обнаруживать все истинные положительные случаи, что делает его полезным в задачах, где пропуск положительных случаев нежелателен.
    
    Недостатки Recall:
    
    Не учитывает ложно-положительные результаты: Recall не учитывает ложно-положительные результаты, что может привести к недооценке качества модели в случае, если они также имеют высокую стоимость.
    
    ---
    
    Метрика **F1** (F1 Score) - это гармоническое среднее между Precision и Recall, которое представляет собой компромисс между ними. F1 Score объединяет Precision и Recall в одну метрику и обычно используется в задачах классификации для оценки сбалансированности модели.
    
    Достоинства F1 Score:
    
    Учитывает как Precision, так и Recall: F1 Score учитывает как точность модели в предсказании положительных классов (Precision), так и способность модели обнаруживать все истинные положительные случаи (Recall).
    
    Недостатки F1 Score:
    
    Не учитывает важность Precision и Recall: F1 Score представляет собой компромисс между Precision и Recall, но не учитывает их относительную важность в конкретной задаче.
    
    ---
    
    **PR AUC** (Precision-Recall Area Under the Curve) - это метрика, используемая в задачах классификации машинного обучения для измерения площади под кривой "precision-recall". Она обеспечивает комплексную оценку эффективности модели на различных уровнях precision и recall при различных пороговых значениях классификации.
    
    Когда используется?
    PR AUC используется, когда необходимо оценить эффективность модели классификации при различных компромиссах между precision и recall и выбрать оптимальное значение порога. Он особенно полезен, когда набор данных несбалансирован.
    
    ---
    
    **ROC AUC** (Receiver Operating Characteristic Area Under the Curve) - это метрика, используемая в задачах классификации машинного обучения для измерения площади под кривой receiver operating characteristic (ROC). Она оценивает компромисс между частотой истинных и ложных положительных результатов при различных пороговых значениях классификации.
    
- 34. Метрики эффективности моделей регрессии. Виды, характеристика, выбор.
    
    **MSE** (Mean Squared Error) - это метрика оценки качества модели в задачах регрессии. Она измеряет среднеквадратичную разницу между предсказанными значениями модели и их истинными значениями. Для каждого примера разница между предсказанным и истинным значением возводится в квадрат, затем суммируется и делится на количество примеров.
    
    Достоинства MSE:
    
    Отражение величины ошибки: MSE позволяет оценить, насколько сильно отклоняются предсказанные значения от истинных, причем большие отклонения учитываются сильнее, чем меньшие.
    
    Недостатки MSE:
    Чувствительность к выбросам: Большие ошибки (выбросы) могут существенно увеличить MSE, делая его менее устойчивым к выбросам.
    Обычно используется в ситуациях, когда большие ошибки считаются более критичными
    
    ---
    
    **RMSE** (Root Mean Squared Error) - это метрика оценки качества модели в задачах регрессии, которая измеряет среднеквадратическое отклонение между предсказанными значениями модели и их истинными значениями, а затем берет квадратный корень из этого значения.
    
    Когда используется RMSE:
    RMSE часто используется в тех случаях, когда важно понять, насколько хорошо модель предсказывает значения в исходных единицах измерения.
    
    Достоинства RMSE:
    1. Учет величины ошибок: RMSE учитывает величину отклонений между предсказанными и истинными значениями, так как она возводит ошибки в квадрат, что делает эту метрику более чувствительной к большим отклонениям.
    2. Интерпретируемость: RMSE также легко интерпретировать, так как она находится в тех же единицах измерения, что и исходные данные.
    
    Недостатки RMSE:
    
    Чувствительность к выбросам: RMSE также чувствителен к выбросам, как и MSE, что может привести к искаженным результатам в присутствии выбросов.
    
    ---
    
    **MAE** (Mean Absolute Error) - это метрика оценки качества модели в задачах регрессии. Она измеряет среднее абсолютное отклонение между предсказанными значениями модели и их истинными значениями.
    
    Когда используется MAE:
    Полезно, когда все ошибки, независимо от их размера, одинаково важны
    
    Достоинства MAE:
    1. Устойчивость к выбросам: MAE более устойчив к выбросам, чем MSE, так как не возводит ошибки в квадрат.
    2. Простота интерпретации: MAE легко интерпретировать - это среднее абсолютное отклонение предсказанных значений от истинных.
    
    Недостатки MAE:
    
    Менее чувствительна к большим ошибкам: Поскольку MAE не учитывает величину отклонения, она может быть менее чувствительной к большим ошибкам по сравнению с MSE.
    
    ---
    
    **R-квадрат** (R2) - это статистический показатель, отражающий долю дисперсии зависимой переменной, которая объясняется независимыми переменными в регрессионной модели. Он часто используется в качестве метрики для оценки хорошего соответствия регрессионной модели.
    
    R2 **оценивает, насколько хорошо регрессионная модель соответствует фактическим данным**
    
    Когда используется?
    R2 обычно используется для оценки эффективности регрессионных моделей. Он дает представление о том, насколько хорошо независимые переменные объясняют изменчивость зависимой переменной. Более высокие значения R2 указывают на то, что независимые переменные лучше объясняют дисперсию зависимой переменной.
    
    Преимущества:
    
    Простая интерпретация: Значения R2 варьируются от 0 до 1, где 1 означает идеальное соответствие, а 0 - то, что модель не объясняет дисперсию зависимой переменной.
    
    Недостатки:
    1. Эта метрика не определена, если y = const
    2. Чувствителен к выбросам: R2 чувствителен к выбросам, и экстремальные значения могут непропорционально сильно влиять на его значение.
    
    ---
    
    **MSLE** (Mean Squared Logarithmic Error) - метрика, используемая в задачах регрессии, особенно когда целевая переменная охватывает несколько порядков величины.
    
    Когда она используется?
    MSLE обычно используется, когда диапазон целевой переменной велик и необходимо штрафовать ошибки в логарифмическом масштабе. Это особенно полезно при прогнозировании величин, изменяющихся на несколько порядков, таких как цены или численность населения.
    
    Преимущества:
    
    Работа с большими диапазонами: MSLE эффективен для задач регрессии, где целевая переменная охватывает несколько порядков величины.
    
    Недостатки:
    
    Интерпретируемость: MSLE менее интерпретируема, чем другие метрики, такие как MAE или RMSE, поскольку она оперирует логарифмической шкалой ошибок.
    
    ---
    
    **MAPE** означает Mean Absolute Percentage Error, метрика, используемая в задачах регрессии для измерения точности предсказаний в процентах от фактических значений.
    
    Когда используется?
    MAPE обычно используется в задачах прогнозирования и регрессии, особенно в бизнесе и финансах, где важно понимание процентной ошибки по отношению к фактическим значениям. Он дает представление о средней величине ошибок по отношению к фактическим значениям.
    
    Преимущества:
    1. Интерпретируемость: MAPE легко интерпретировать, поскольку он представляет собой среднюю процентную ошибку по отношению к фактическим значениям.
    2. Отражает относительную погрешность: MAPE отражает относительную величину ошибок, что может быть информативным, особенно в приложениях, где процентная ошибка более критична, чем абсолютная.
    
    Недостатки:
    1. Не подходит для нулевых или близких к нулю фактических значений: MAPE нельзя рассчитать, если фактические значения близки к нулю, так как он предполагает деление на ноль.
    2. Чувствительность к выбросам: MAPE может быть чувствителен к выбросам, особенно если фактические значения малы или сильно различаются.
    
    ---
    
    Метрика **Absolute Median Error** (AME) - это метрика оценки качества модели в задачах регрессии, которая измеряет медианное абсолютное отклонение между предсказанными значениями модели и их истинными значениями.
    
    Когда используется AME:
    AME используется в тех случаях, когда важно оценить среднюю абсолютную ошибку модели, учитывая медиану вместо среднего значения. Это может быть полезно, если данные содержат выбросы.
    
    Достоинства AME:
    
    Устойчивость к выбросам: AME менее чувствителен к выбросам, чем среднее абсолютное отклонение (MAE), так как использует медиану вместо среднего значения.
    
    Недостатки AME:
    
    Менее чувствительный к большим отклонениям: Поскольку AME использует медиану, он может быть менее чувствительным к большим отклонениям в данных по сравнению с средним значением.
    
    ---
    
    **Максимальная ошибка** - это метрика, используемая в задачах регрессии для измерения наибольшего отклонения между прогнозируемыми и фактическими значениями.
    
    Когда она используется?
    Максимальная ошибка используется, когда необходимо понять наихудший сценарий или максимальное расхождение между предсказанными и фактическими значениями. Он дает представление о наибольшей ошибке, которую потенциально может допустить модель.
    
    Преимущества:
    1. Определяет наихудшие сценарии: Максимальная ошибка выделяет максимальное отклонение между прогнозируемыми и фактическими значениями, помогая выявить наиболее значительные ошибки, допущенные моделью.
    2. Простота интерпретации: ее легко понять, так как она представляет собой наибольшее расхождение между прогнозируемыми и фактическими значениями.
    
    Недостатки:
    
    Нечувствительность к общей эффективности модели: Максимальная ошибка не дает информации об общей эффективности модели за пределами одной наибольшей ошибки.
    
    ---
    
    **Выбор метрики**:
    
    Выбор метрики эффективности в задачах регрессии зависит от нескольких факторов, включая специфику данных, цели моделирования и требований бизнеса. Вот несколько шагов, которые помогут выбрать подходящую метрику:
    
    1. Понимание задачи:
       - Определите цель вашей модели регрессии. Например, вы хотите предсказывать цены на недвижимость, количество проданных товаров или результаты медицинских тестов?
    
    2. Выбор метрик:
       - Рассмотрите различные метрики и их соответствие вашим целям. Например:
         - MAE (Mean Absolute Error): хорошо подходит для оценки среднего абсолютного отклонения предсказаний от истинных значений. Это хорошо подходит для сценариев, где все ошибки одинаково важны.
         - MSE (Mean Squared Error): учитывает квадратичные ошибки и может быть полезен, если важно учитывать большие отклонения.
         - RMSE (Root Mean Squared Error): это квадратный корень из MSE и выражен в тех же единицах, что и целевая переменная, что делает его интерпретируемым.
         - R-squared (Coefficient of Determination): измеряет долю дисперсии в целевой переменной, которая объясняется моделью. Чем ближе к 1, тем лучше модель.
         - MAPE (Mean Absolute Percentage Error): полезен для измерения процентного отклонения предсказаний от истинных значений.
    
    3. Сравнение метрик:
       - Сравните различные метрики на вашем наборе данных и выберите ту, которая наилучшим образом отражает качество модели с учетом специфики вашей задачи и требований бизнеса.
    
- 35.Перекрестная проверка (кросс-валидация). Назначение, схема работы.
    
    **Ответ:**
    
    - Кросс-валидация — это процесс проверки качества модели, для которого используют все доступные данные, кроме отложенной тестовой выборки.
    - Во время кросс-валидации модель несколько раз обучается и проверяется. Для этого тренировочные данные делят на несколько блоков. Каждый из них выступит в качестве валидационной выборки на одном из циклов проверки модели. Количество циклов равно числу выделенных блоков. В конце алгоритм кросс-валидации считает среднее арифметическое значений метрик, полученных на каждом цикле проверки модели.
    - Помимо среднего арифметического, полезно рассчитать стандартное отклонение оценок качества модели, чтобы узнать, насколько стабильны её предсказания на любых данных.
    - Вы можете разбить данные на блоки самостоятельно с помощью `KFold` и `StratifiedKFold` или же довериться `cross_val_score()` для кросс-валидации модели.
    
    ![Untitled](%D0%9C%D0%B0%D1%88%D0%B8%D0%BD%D0%BD%D0%BE%D0%B5%20%D0%BE%D0%B1%D1%83%D1%87%D0%B5%D0%BD%D0%B8%D0%B5%20%D1%8D%D0%BA%D0%B7%D0%B0%D0%BC%D0%B5%D0%BD%2004f25e7078ad4106b73d3ba8d4abfdb3/Untitled%2019.png)
    
    **KFold разбиение**
    
    В k-блочной перекрёстной проверке исходные данные разбиваются на 𝑘 (примерно) равных по количеству частей, называемых "блоками", на 𝑘−1 из которых производится обучение, а на 1 валидация. В результате получается более робастная оценка эффективности выбранной модели.
    
    **StratifiedKFold разбиение**
    
    Метод StratifiedKFold — это метод KFold, использующий стратификацию при разбиении на фолды: каждый фолд содержит примерно такое же соотношение классов, как и всё исходное множество. Такой подход может потребоваться в случае, например, очень несбалансированного соотношения классов.
    
    **Leave-one-out**
    
    Метод leave-one-out (LOO) является частным случаем метода k-Fold: в нём каждый фолд состоит ровно из одного семпла.
    
- 36.Конвейеры в библиотеке sklearn. Назначение, использование.
    
    **Ответ:**
    
    **Пайплайн** (от англ. pipeline — «конвейер, трубопровод»), или **конвейер**, — это инструмент, который позволяет автоматизировать действия по подготовке данных, обучению моделей и оценке их качества. 
    
    Использование конвейеров в sklearn очень просто. Для создания конвейера необходимо создать экземпляр класса Pipeline и передать ему список кортежей, каждый из которых содержит имя шага и экземпляр класса, реализующего этот шаг. Например:
    
    ```python
    from sklearn.pipeline import Pipeline
    from sklearn.preprocessing import StandardScaler
    from sklearn.linear_model import LinearRegression
    
    pipeline = Pipeline([
        ('scaler', StandardScaler()),
        ('model', LinearRegression())
    ])
    
    ```
    
    В этом примере создается конвейер, состоящий из двух шагов: нормализации признаков с помощью StandardScaler и обучения линейной регрессии с помощью LinearRegression.
    
    Для применения конвейера к данным необходимо вызвать метод fit или predict у экземпляра класса Pipeline, передав ему соответствующие данные. Например:
    
    ```python
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
    
    pipeline.fit(X_train, y_train)
    y_pred = pipeline.predict(X_test)
    
    ```
    
    В этом примере конвейер применяется к тренировочным данным с помощью метода fit, а затем к тестовым данным с помощью метода predict.
    
- 37. Использование методов визуализации данных для предварительного анализа.
    
    Методы визуализации данных играют важную роль в предварительном анализе данных перед применением методов машинного обучения. Визуализация данных позволяет исследовать структуру данных, выявлять закономерности, обнаруживать выбросы и пропущенные значения, а также понимать взаимосвязи между признаками.
    
    Matplotlib является частью Scientific Python — набора библиотек для научных вычислений и визуализации данных, куда также входят **NumPy , SciPy, Pandas , SymPy** и ещё ряд других инструментов.
    
    **Seaborn** — это библиотека для решения задач визуализации с ориентацией на работу в области статистики. Базой seaborn является библиотека Matplotlib, о которой подробно было рассказано в первой части книги.
    
    **Инструменты** для визуализации данных, которые предоставляет seaborn можно разделить на пять групп:
    
    - визуализация отношений в данных;
    - визуализация категориальных данных;
    - визуализация распределений;
    - визуализация линейной регрессии;
    - визуализация матричных наборов данных.
    
    Некоторые из распространенных методов визуализации данных для предварительного анализа включают в себя:
    
    1. Диаграммы рассеяния (**scatter plots**) для визуализации взаимосвязей между двумя переменными.
    
    2. Гистограммы (**histograms**) для изучения распределения отдельных признаков.
    
    3. Ящики с усами (**box plots**) для выявления выбросов и размаха значений признаков.
    
    4. Графики корреляции (correlation plots) для оценки корреляций между признаками.
    
    5. Парные диаграммы рассеяния (pair plots) для исследования взаимосвязей между несколькими парами признаков.
    
    Эти методы помогают лучше понять данные, подготовить их к обучению модели, а также выбрать подходящие методы машинного обучения для конкретной задачи. Визуализация данных является мощным инструментом для исследования и понимания данных до того, как приступать к построению модели машинного обучения.
    
- 38. Исследование коррелированности признаков: методы, цели, выводы.
    
    Исследование коррелированности признаков в машинном обучении является важным этапом в предобработке данных. **Цель** этого исследования заключается в **выявлении зависимостей между признаками**, чтобы избежать избыточности информации или мультиколлинеарности, что может негативно сказаться на качестве модели.
    
    Для анализа коррелированности признаков применяют различные методы, т**акие как матрица корреляции, коэффициент корреляции Пирсона, коэффициент корреляции Спирмена и др**. После проведения анализа можно делать выводы о том, какие признаки сильно коррелируют между собой, и исключать из модели избыточные признаки.
    
    **Выводы** из исследования коррелированности признаков могут помочь **оптимизировать модель** машинного обучения, повысить её точность и устойчивость к переобучению. Также они могут помочь в **выборе** наиболее информативных **признаков** для построения модели.
    
    Одним из наиболее развернутых способов, позволяющих оценить качество бинарной классификации, является использование матрицы ошибок. Давайте исследуем прогнозы модели LogisticRegression, построенной в предыдущем разделе, с помощью функции confusion_matrix.
    
- 39. Решкалирование данных. Виды, назначение, применение. Нормализация и стандартизация данных.
    
    **Решкалирование данных**
    
    Решкалирование данных — это процесс преобразования признаков данных в такие диапазоны, которые способствуют улучшению производительности моделей машинного обучения. Цель решкалирования — устранение различий в масштабах различных признаков, что позволяет моделям быстрее и эффективнее обучаться.
    
    **Виды решкалирования данных**
    
    1. Нормализация (Normalization)
    2. Стандартизация (Standardization)
    
    **Нормализация**
    
    *Определение:*
    
    Нормализация — это процесс преобразования значений признаков в диапазон от 0 до 1 (или от -1 до 1). Обычно используется минимум и максимум признака для приведения данных к такому диапазону.
    
    *Формула:*
    
    ![Untitled](%D0%9C%D0%B0%D1%88%D0%B8%D0%BD%D0%BD%D0%BE%D0%B5%20%D0%BE%D0%B1%D1%83%D1%87%D0%B5%D0%BD%D0%B8%D0%B5%20%D1%8D%D0%BA%D0%B7%D0%B0%D0%BC%D0%B5%D0%BD%2004f25e7078ad4106b73d3ba8d4abfdb3/Untitled%2020.png)
    
    где x′ — нормализованное значение признака, x — исходное значение признака, xmin⁡ и xmax — минимальное и максимальное значения признака соответственно.
    
    *Назначение:*
    
    - Применяется, когда необходимо привести данные к единому масштабу.
    - Полезна для алгоритмов, чувствительных к масштабам данных, таких как метод ближайших соседей (k-NN) и нейронные сети.
    
    **Стандартизация**
    
    *Определение:*
    
    Стандартизация — это процесс преобразования данных таким образом, что они имеют свойства стандартного нормального распределения с нулевым средним значением и единичной дисперсией.
    
    *Формула:*
    
    ![Untitled](%D0%9C%D0%B0%D1%88%D0%B8%D0%BD%D0%BD%D0%BE%D0%B5%20%D0%BE%D0%B1%D1%83%D1%87%D0%B5%D0%BD%D0%B8%D0%B5%20%D1%8D%D0%BA%D0%B7%D0%B0%D0%BC%D0%B5%D0%BD%2004f25e7078ad4106b73d3ba8d4abfdb3/Untitled%2021.png)
    
    где x′ — стандартизированное значение признака, x — исходное значение признака, μ — среднее значение признака, σ — стандартное отклонение признака.
    
    *Назначение:*
    
    - Применяется, когда признаки имеют разные масштабы и распределения.
    - Полезна для алгоритмов, таких как линейная регрессия, логистическая регрессия, SVM и k-средних.
    
    **Применение решкалирования данных**
    
    *Когда использовать нормализацию:*
    
    - Когда признаки имеют разный масштаб, и требуется привести их к одному диапазону.
    - Когда используются алгоритмы, чувствительные к масштабу данных (например, k-NN, нейронные сети).
    
    *Когда использовать стандартизацию:*
    
    - Когда данные имеют разное распределение, и необходимо приведение данных к стандартному нормальному распределению.
    - Когда используются алгоритмы, предполагающие нормальное распределение данных (например, линейная регрессия, логистическая регрессия).
- 40. Преобразование категориальных признаков в числовые.
    
    **Основные методы преобразования категориальных признаков**
    
    1. Порядковое кодирование (Ordinal Encoding)
    2. One-Hot кодирование (One-Hot Encoding)
    3. Двоичное кодирование (Binary Encoding)
    
    **Порядковое кодирование (Ordinal Encoding)**
    
    Порядковое кодирование преобразует категориальные признаки в числовые, присваивая каждой категории уникальное числовое значение. Подходит для признаков, где существует естественный порядок.
    
    *Пример:*
    
    Категория: ["низкий", "средний", "высокий"] → [0, 1, 2]
    
    **One-Hot кодирование (One-Hot Encoding)**
    
    One-Hot кодирование преобразует категориальные признаки в бинарные столбцы, где каждая категория представлена отдельным столбцом, принимающим значение 1 или 0. Подходит для признаков без естественного порядка.
    
    ![Untitled](%D0%9C%D0%B0%D1%88%D0%B8%D0%BD%D0%BD%D0%BE%D0%B5%20%D0%BE%D0%B1%D1%83%D1%87%D0%B5%D0%BD%D0%B8%D0%B5%20%D1%8D%D0%BA%D0%B7%D0%B0%D0%BC%D0%B5%D0%BD%2004f25e7078ad4106b73d3ba8d4abfdb3/Untitled%2022.png)
    
    **Двоичное кодирование (Binary Encoding)**
    
    Двоичное кодирование сначала применяет порядковое кодирование, а затем преобразует числовые значения в двоичную систему счисления и разделяет биты на отдельные столбцы.
    
    *Пример:*
    
    Категория: ["a", "b", "c"] → [0, 1, 2] → [000, 001, 010]
    
- 41. Методы визуализации данных для машинного обучения.
    
    Методы визуализации данных играют важную роль в подготовке данных для применения методов машинного обучения. Визуализация позволяет исследовать структуру данных, выявлять закономерности, обнаруживать аномалии и выбросы, а также понимать взаимосвязи между признаками. Некоторые методы визуализации данных для машинного обучения включают в себя:
    
    1. Диаграммы рассеяния (Scatter Plots): Позволяют визуально оценить взаимосвязь между двумя переменными. Это полезно для определения корреляций и паттернов в данных.
    
    2. Гистограммы (Histograms): Используются для изучения распределения отдельных признаков. Позволяют оценить форму распределения и выявить выбросы.
    
    3. Ящики с усами (Box Plots): Показывают размах значений признака, а также выявляют выбросы и медианные значения.
    
    4. Графики корреляции (Correlation Plots): Позволяют оценить корреляцию между признаками. Это важно для понимания взаимосвязей между переменными.
    
    5. Парные диаграммы рассеяния (Pair Plots): Позволяют исследовать зависимости между несколькими парами признаков одновременно. Это полезно для обнаружения общих закономерностей.
    
    Использование этих методов визуализации данных помогает лучше понять данные, выявить проблемы в данных, выбрать подходящие признаки для модели машинного обучения и улучшить процесс обучения модели.
    
- 42. Задача выбора модели. Оценка эффективности, валидационный набор.
    
    Выбор модели в машинном обучении — это критически важный этап, включающий в себя оценку различных моделей и выбор наилучшей из них для конкретной задачи. Этот процесс включает несколько шагов:
    
    ### Шаг 1: Определение задачи
    
    Для начала необходимо чётко определить задачу, которую должна решать модель. Это может быть задача классификации, регрессии, кластеризации и т.д.
    
    ### Шаг 2: Подбор метрик эффективности
    
    Метрики эффективности играют ключевую роль в оценке качества модели. Примеры таких метрик:
    
    - **Для задач классификации:** точность (accuracy), полнота (recall), точность (precision), F1-мера и ROC-AUC.
    - **Для задач регрессии:** среднеквадратичная ошибка (MSE), средняя абсолютная ошибка (MAE), R².
    
    ### Шаг 3: Разделение данных
    
    Для объективной оценки модели необходимо разделить данные на три набора:
    
    1. **Обучающий набор (Training Set):** используется для обучения модели.
    2. **Валидационный набор (Validation Set):** используется для оценки модели на промежуточных этапах обучения и выбора гиперпараметров.
    3. **Тестовый набор (Test Set):** используется для окончательной оценки модели.
    
    ### Шаг 4: Обучение и валидация модели
    
    Процесс обучения модели включает оптимизацию параметров на обучающем наборе. В процессе валидации на валидационном наборе происходит оценка различных гиперпараметров модели, таких как коэффициенты регуляризации, глубина деревьев решений и т.д.
    
    ### Шаг 5: Выбор лучшей модели
    
    На основе результатов на валидационном наборе выбирается модель, показавшая наилучшие результаты по заданным метрикам. Важно не переобучиться на валидационном наборе, поэтому используется кросс-валидация — метод, при котором данные несколько раз разбиваются на обучающие и валидационные наборы, и результаты усредняются.
    
    ### Шаг 6: Окончательная оценка на тестовом наборе
    
    После выбора модели, её качество проверяется на тестовом наборе данных, который не участвовал в процессе обучения и валидации. Это позволяет получить объективную оценку производительности модели на новых данных.
    
- 43. Кривые обучения для диагностики моделей машинного обучения.
    
    **Кривая обучения** - это всего лишь **график, показывающий прогресс в освоении определенного показателя, связанного с обучением, во время обучения** модели машинного обучения. Это всего лишь математическое представление процесса обучения.
    
    Мы часто видим эти два типа кривых обучения на графиках:
    
    - *Кривые обучения оптимизации*: кривые обучения, рассчитываемые на основе показателя, с помощью которого оптимизируются параметры модели, такие как потери или среднеквадратичная ошибка
    - *Кривые обучения производительности*: кривые обучения, рассчитываемые на основе показателя, по которому будет оцениваться и выбираться модель, такого как точность, прецизионность, отзыв или оценка F1
    
    **PR-AUC**
    
    Кривая precision-recall используется для методов метрической классификации, которые выдают вероятность принадлежности объекта данному классу.
    
    Дискретная классификации производится при помощи порогового значения. Чем больше порог, тем больше объектов модель будет относить к отрицательному классу. Повышение порога в среднем увеличивает precision модели, но понижает recall.
    
    - PR-кривая используется чтобы выбрать оптимальное значение порога.
    - PR-кривая нужна для того, чтобы сравнивать и оценивать модели вне зависимости от выбранного уровня порога.
    - PR-AUC - площадь под PR-кривой, у лучшей модели - 1.0, у тривиальной - 0.5, у худшей - 0.0
    
    **ROC-AUC**
    
    ROC-кривая показывает качество бинарной классификации при разных значениях порога. В отличие от PR-кривой, ROC-кривая монотонна.
    
    Площадь под графиком ROC-кривой, ROC_AUC - одна из основных метрик качества классификационных моделей.
    
    ROC_AUC можно использовать для сравнения качества разных моделей, обученных на разных данных. ROC чаще используют для сбалансированных и множественных задач, PR - для несбалансированных.
    
    Кривые для множественной классификации строятся *отдельно* для каждого класса.
    
    Метрика AUC считается по кривой средних значений.
    
- 44. Регуляризация моделей машинного обучения. Назначение, виды, формализация.
    
    https://python-school.ru/blog/osnovy-ml/regularization-l1-l2/
    
    **Регуляризация** - это метод, используемый для предотвращения переобучения путем добавления штрафного условия к целевой функции модели во время обучения. Цель состоит в том, чтобы помешать модели слишком точно соответствовать данным обучения и продвигать более простые модели, которые лучше обобщают невидимые данные. Методы регуляризации позволяют контролировать сложность моделей, снижая значения больших коэффициентов или выбирая подмножество признаков, что помогает найти правильный баланс между смещением и дисперсией.
    
    Типы Регуляризации
    
    **Регуляризация L1 (лассо):**
    
    Л1(или регрессия лассо), популярный метод регуляризации в машинном обучении, предлагает общедоступный подход для предотвращения переобучения и выполнения функций выбора при регрессионном моделировании .Традиционные регрессионные модели могут давать сбои при работе с многомерными наборами данных, сохраняя множество нерелевантных функций, что приводит к низкой производительности прогнозирования и интерпретируемости модели.   Регрессия Лассо решает эти проблемы, вводя штрафной член в функцию потерь, стимулируя разреженность модели и выбирая только наиболее подходящие предикторы для включения переменной.
    
    На основе регрессии Лассо лежит ее способность выполнять автоматический выбор признаков, сводя некоторые оценки коэффициентов точно к вероятности. Добавление к функциям потерь штрафного члена, пропорциональных абсолютных значений коэффициентов, регрессии Лассо показывает большие значения коэффициентов и дополнительную разреженность моделей.Этот метод регуляризации особенно полезен при работе с многомерными наборами данных, где он помогает упрощенной модели и повышению интерпретируемости, концентрируя внимание на
    
    Преимущества:
    
    Одним из главных преимуществ регрессии Lasso является ее способность эффективно обрабатывать наборы данных с большим количеством предсказателей. В схемах, где присутствует множество функций, традиционные регрессионные модели могут страдать от проклятия размерности, что приводит к снижению производительности прогнозирования и повышению вычислительной сложности. Регрессия Лассо преодолевает это ограничение, автоматически выбирая наиболее релевантные функции, что приводит к созданию более простых и интерпретируемых моделей.
    
    **Регуляризация L2 (Ridge):**
    
    Ребристая регрессия, Всемирный метод регуляризации в области машинного обучения, предлагает надежное решение для предотвращения переобучения и улучшения обобщения моделей. Традиционные регрессионные модели, такие как линейная регрессия, могут давать сбои при работе с наборами данных, содержащих мультиколлинеарные объекты, где предсказатели имеют основную закономерность. Это явление часто приводит к нестабильным коэффициентам прироста и низкой производительности прогнозов. Ребристая регрессия решает эти проблемы, вводя штрафной член в функцию потерь, способствуя уменьшению результатов коэффициентов и упрощению моделей.
    
    Основной принцип, основанный на рестойбри регрессии, заключается в ее способности находить баланс между взлётностью и дисперсией.Добавление к функции потери штрафного члена, пропорционального квадрата коэффициентов, ребристая регрессия эффективно приводит к оценке коэффициентов, сохраняя при этом их ненулевое значение.Этот метод регуляризации особенно используется при работе с коллинеарными наборами данных, где он помогает стабилизировать модель для расчета, снижая чувствительность коэффициентов к небольшим изменениям в обучающих данных.
    
    Преимущества:
    
    Одним из главных преимуществ гребневой регрессии является ее способность корректно обрабатывать мультиколлинеарность.В результате, когда предсказатели имеют базовую закономерность, традиционные регрессионные модели могут давать нестабильные и ненадежные оценки коэффициентов.Ребристая регрессия преодолевает это ограничение, ограничивая самые большие коэффициенты, тем самым повышая модель стабильности.Кроме того, ребристая регрессия имеет влияние на хорошую работу, когда количество предикторов приводит к изменению количества, что является обычным сценарием для многих наборов данных.
    
    Регуляризация эластичной сети:
    
    Регуляризация эластичной сети Гибридный подход, сочетающий методы регрессии гребня и лассо, предлагает универсальное решение для переобучения и выполнения функций выбора при регрессионном моделировании. Традиционные регрессионные модели могут сталкиваться с проблемами при работе с наборами данных, содержащими мультиколлинеарные объекты и основную размерность, где баланс сложности и разреженности моделей имеет решающее значение для адаптивной производительности. Регуляризация эластичной сети решает эти проблемы, добавляя штрафные условия L1 и L2 к потерям функций, создавая гибкую схему для контроля компромисса между коэффициентом уменьшения и функцией выбора.
    
    Ключевым принципом регулирования эластичной сети является достижение баланса между методами регрессии гребня и лассо. Добавление штрафных терминов L1 и L2 к потерям функций, эластичная сетевая регулярность сочетается с сильными сторонами обоих подходов, смягчая их ограничения. Этот гибридный подход обеспечивает повышенную гибкость и адаптивность, позволяя специалистам-практикам адаптировать регуляризацию к использованию наборов данных по характеристикам.
    
    Преимущества:
    
    Одним из главных преимуществ эластичной сетевой регуляризации является ее способность эффективно обрабатывать наборы данных в рамках цифровой структуры.В связи с этим, где присутствуют мультиколлинеарность и размерность, сохранение высоких регрессионных моделей может быть трудно найти баланс между классификацией моделей и разреженностью.Регуляризация эластичной сети преодолевает это ограничение, обеспечивая единую структуру, которая легко интегрирует методы регрессии гребня и лассо, что приводит к созданию более надежных моделей прогнозирования.
    
- 45.Проблема сбора и интеграции данных для машинного обучения.
    
    Проблема сбора и интеграции данных для машинного обучения является одной из наиболее сложных и важных задач. Данные являются основой для обучения моделей, и качество, и количество данных во многом определяют эффективность и точность работы системы.
    
    Сбор данных для машинного обучения может быть связан с рядом трудностей, таких как:
    
    - Доступность данных: иногда данные, необходимые для обучения модели, могут быть недоступны или доступны только в ограниченном количестве.
    - Качество данных: данные, которые используются для обучения модели, должны быть точными, полными и соответствовать требованиям задачи. Иногда данные могут содержать ошибки, пропуски или шум, что может отрицательно сказаться на качестве обученной модели.
    - Конфиденциальность и безопасность данных: при сборке и обработке данных необходимо соблюдать требования законодательства и нормативных актов в области конфиденциальности и безопасности данных.
    
    Интеграция данных для машинного обучения также может быть связана с рядом трудностей, таких как:
    
    - Несоответствие форматов и структур данных: данные, которые используются для обучения модели, могут быть представлены в различных форматах и структурах, что может требовать дополнительной обработки и преобразования данных.
    - Несоответствие единиц измерения: данные, которые используются для обучения модели, могут быть представлены в различных единицах измерения, что может требовать дополнительной обработки и преобразования данных.
    - Несоответствие временных интервалов: данные, которые используются для обучения модели, могут быть собраны в различные временные интервалы, что может требовать дополнительной обработки и преобразования данных.
    
    Для решения проблем сбора и интеграции данных для машинного обучения могут быть использованы различные подходы и инструменты, такие как:
    
    - Автоматизированный сбор данных: применение специализированных программных инструментов для сбора данных с различных источников, таких как веб-сайты, социальные сети, датчики и т.д.
    - Обработка и преобразование данных: применение специализированных программных инструментов для обработки и преобразования данных, таких как библиотеки Pandas и NumPy в Python.
    - Машинное обучение на неполных данных: применение специализированных алгоритмов машинного обучения, способных работать с неполными или неточными данными.
    - Использование систем управления базами данных: применение специализированных программных инструментов для хранения, обработки и интеграции данных, таких как MySQL, PostgreSQL, Oracle и т.д.
    - Использование облачных платформ для хранения и обработки данных: применение специализированных облачных платформ для хранения и обработки данных, таких как Amazon Web Services, Microsoft Azure, Google Cloud Platform и т.д.
- 46.Понятие чистых данных и требования к данным.
    
    **Понятие чистых данных**
    
    1. Датасет - это набор данных, используемый для обучения моделей. Данные представлены в виде единой таблицы.
    2. Объекты - это элементарные сущности, которые мы изучаем, объекты реального мира, измерения, наблюдения. В датасете представляются строками.
    3. Каждый объект характеризуется набором атрибутов. В датасете атрибуты представляются столбцами.
    4. Каждая таблица, файл представляет собой данные об одном виде наблюдений или экспериментов и должна иметь внутреннюю согласованность.
    5. Дополнительно: Все данные должны быть выражены в численном виде.
    6. Дополнительно: В данных не должно быть отсутствующих (пропущенных) значений.
    7. Существуют специальные структуры данных и форматы.
    
    **Оценка источников и объемов данных**
    
    1. После постановки задачи машинного обучения первый этап моделирования - определение источников и объема данных, необходимых для эффективного обучения.
    2. Объем данных определяется порядком сложности задачи: чем больше данных, тем более сложные модели можно на них строить.
    3. В целом, чем больше данных можно собрать, тем лучше.
    4. Данные в датасете должны быть репрезентативной выборкой генеральной совокупности.
    5. Источники данных могут быть открытые и закрытые, платные, по подписке, внутренние и внешние.
    6. Данные могут быть пакетные и потоковые, с ними надо работать по разному.
    7. Данные можно не только собирать, но еще генерировать и модифицировать.
    8. Существующие данные может потребоваться размечать.
- 47. Основные задачи описательного анализа данных.
    
    *Сомнительный ответ*😑
    
    [What is Descriptive Analytics and how does it summarize past data in a simple way? - GeeksforGeeks (turbopages.org)](https://translated.turbopages.org/proxy_u/en-ru.ru.29aa8fca-666de424-3e1ce8e3-74722d776562/https/www.geeksforgeeks.org/what-is-descriptive-analytics-and-how-does-it-summarize-past-data-in-a-simple-way/)
    
    Описательный анализ данных (также известный как описательная статистика) — это первый этап в изучении данных, который включает в себя сбор, обработку и описание данных. Основные задачи описательного анализа данных включают:
    
    1. Сбор и структурирование данных: Сбор данных из различных источников и их структурирование в удобной для анализа форме, например, в таблицах или базах данных.
    2. Очистка данных: Удаление или исправление неверных, неполных или несоответствующих данных, чтобы обеспечить качество данных для последующего анализа.
    3. Описательная статистика: Расчет статистических показателей, таких как среднее значение, медиана, мода, размах, дисперсия, стандартное отклонение, чтобы получить первоначальное представление о распределении данных и их центральных тенденциях.
    4. Графическое представление данных: Создание графиков и диаграмм (например, гистограмм, ящиков с усами, точечных графиков), которые помогают визуализировать данные и выявлять закономерности, аномалии или тенденции.
    5. Группировка и классификация данных: Организация данных в группы или классы на основе определенных критериев, чтобы упростить анализ и интерпретацию.
    6. Определение взаимосвязей между переменными: Исследование корреляций или связей между различными переменными, что может помочь в понимании, как одни факторы влияют на другие.
    7. Подготовка отчетов: Составление отчетов или презентаций, которые обобщают результаты описательного анализа, чтобы предоставить информацию заинтересованным сторонам.
    
    Описательный анализ данных является фундаментальным шагом в статистическом анализе, поскольку он предоставляет базовую информацию о данных, необходимую для более сложных методов анализа, таких как статистическое выводы или прогнозирование.
    
- 48. Полиномиальные модели машинного обучения.
    
    Полиномиальные модели машинного обучения представляют собой тип моделей, которые используют полиномиальные функции для описания взаимосвязи между входными переменными (признаками) и выходными переменными (целевыми переменными). Полиномиальные модели могут быть полезны, когда взаимосвязь между входами и выходами нелинейна и не может быть адекватно описана линейными моделями.
    
    В полиномиальных моделях каждый признак может быть возведен в степень, превышающую 1, что позволяет модели учитывать нелинейности в данных. Например, полиномиальная модель второй степени может включать в себя квадратичные и взаимодействия между признаками:
    
    y = b0 + b1 * x1 + b2 * x2 + b3 * x1^2 + b4 * x2^2 + b5 * x1 * x2
    
    где y - целевая переменная, x1 и x2 - входные признаки, а b0, b1, b2, b3, b4 и b5 - коэффициенты модели, которые необходимо оценить с помощью обучающих данных.
    
    Полиномиальные модели могут быть реализованы в рамках различных методов машинного обучения, таких как линейная регрессия с полиномиальными признаками, метод наименьших квадратов, метод опорных векторов (SVM) с полиномиальным ядром и другие.
    
    Однако следует учитывать, что использование полиномиальных моделей может привести к увеличению размерности пространства признаков, что может вызвать переобучение, если обучающий набор данных не достаточно велик. Кроме того, полиномиальные модели могут быть чувствительны к шуму в данных и требовать тщательного выбора степени полинома и регуляризации для предотвращения переобучения.
    
    Полиномиальная регрессия — алгоритм машинного обучения, используемый для прогнозирования.
    
    **Полиномиальная регрессия** — это алгоритм машинного обучения, который используется для моделирования зависимости между зависимой переменной (целевой переменной) и одной или несколькими независимыми переменными (предикторами). В отличие от линейной регрессии, которая предполагает линейную зависимость между переменными, полиномиальная регрессия позволяет моделировать более сложные, нелинейные зависимости.
    
    Алгоритм полиномиальной регрессии основывается на построении полинома, который представляет собой сумму степенных функций независимых переменных. Например, для одной независимой переменной x полиномиальная регрессия может быть представлена в виде:
    
    y = b0 + b1_x + b2_x^2 + ... + bn*x^n
    
    где y — зависимая переменная, x — независимая переменная, b0, b1, ..., bn — коэффициенты регрессии, а n — степень полинома.
    
    Алгоритм полиномиальной регрессии включает следующие шаги:
    
    1. Выбор степени полинома (n): степень полинома определяет сложность модели и количество коэффициентов, которые необходимо оценить. Выбор степени полинома зависит от характера данных и априорной информации о зависимости между переменными.
    2. Генерация признаков: для каждой независимой переменной создаются новые признаки, представляющие собой степени исходной переменной (x, x^2, ..., x^n).
    3. Оценка коэффициентов регрессии: используя метод наименьших квадратов или другой подходящий метод, оцениваются значения коэффициентов b0, b1, ..., bn, которые минимизируют ошибку между предсказанными и фактическими значениями зависимой переменной.
    4. Проверка качества модели: оценивается точность модели, например, с помощью кросс-валидации, RMSE (Root Mean Squared Error) или других метрик. Может потребоваться подбор степени полинома для достижения оптимальной точности.
    5. Использование модели для предсказания: построенная модель полиномиальной регрессии используется для предсказания значений зависимой переменной на основе новых значений независимых переменных.
    
    Важно отметить, что при увеличении степени полинома возможно возникновение переобучения (overfitting), когда модель хорошо описывает обучающие данные, но плохо обобщается на новые данные. Поэтому необходимо тщательно подбирать степень полинома и проверять качество модели на валидационной выборке или с помощью кросс-валидации.
    
- 49. Основные виды преобразования данных для подготовки к машинному обучению.
    
    **1. Масштабирование данных**
    
    Масштабирование данных включает нормализацию и стандартизацию, которые помогают привести признаки к общему масштабу.
    
    - Нормализация: Преобразует данные в диапазон [0, 1] или [-1, 1].
    - Стандартизация: Преобразует данные к нулевому среднему значению и единичной дисперсии.
    
    **2. Преобразование категориальных признаков**
    
    Категориальные признаки необходимо преобразовать в числовые, чтобы использовать их в моделях машинного обучения.
    
    - Порядковое кодирование (Ordinal Encoding): Присваивает числовые значения категориям с порядком.
    - One-Hot кодирование (One-Hot Encoding): Создает бинарные столбцы для каждой категории.
    
    **3. Обработка пропущенных значений**
    
    Пропущенные значения могут значительно влиять на производительность модели, поэтому их необходимо обрабатывать.
    
    - Заполнение средним значением, медианой или модой
    - Удаление строк или столбцов с пропущенными значениями
    
    **4. Обработка выбросов**
    
    Выбросы могут исказить результаты модели, поэтому их необходимо идентифицировать и обрабатывать.
    
    - Удаление выбросов
    - Замена выбросов
    
    **5. Преобразование распределения данных**
    
    Иногда необходимо преобразовать распределение данных, чтобы оно лучше подходило для модели.
    
    - Логарифмическое преобразование
    - Корень квадратный
    
    **6. Преобразование признаков**
    
    Создание новых признаков из существующих может улучшить производительность модели.
    
    - Полиномиальные признаки
    - Взаимодействие признаков
- 50. Задача выбора признаков в машинном обучении.
    
    **Основные цели выбора признаков включают:**
    
    1. Улучшение производительности модели: Удаление нерелевантных или избыточных признаков может повысить точность модели.
    2. Снижение сложности модели: Меньшее количество признаков уменьшает сложность модели, что может сократить время обучения и предсказания.
    3. Повышение интерпретируемости: Меньшее количество признаков облегчает интерпретацию модели.
    
    **Фильтрационные методы (Filter Methods)**
    
    Фильтрационные методы основаны на статистических критериях для оценки значимости признаков. Эти методы выполняются независимо от модели и не используют алгоритм обучения.
    
    - Критерий значимости (Univariate Selection): Использует статистические тесты для выбора наиболее значимых признаков.
    - Корреляционный анализ (Correlation Matrix): Анализ корреляции между признаками и целевой переменной.
    - Variance Threshold: Удаляет признаки, дисперсия которых ниже заданного порога.
    
    **Встроенные методы (Embedded Methods)**
    
    Встроенные методы выполняют выбор признаков в процессе обучения модели. Они интегрированы в алгоритм и используют его собственные критерии значимости.
    
    - L1-регуляризация (Lasso Regression): Добавляет штраф за сумму абсолютных значений коэффициентов, приводя к занулению некоторых коэффициентов.
    - Деревья решений и ансамблевые методы (Decision Trees and Ensemble Methods): Оценка значимости признаков на основе уменьшения неопределённости.
